import argparse
import os
import sqlite3
import re
import struct
import glob
import logging
from contextlib import contextmanager
from datetime import datetime
### --- NEW --- ###
# å¯¼å…¥å¹¶è¡Œå¤„ç†åº“
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
### --- NEW --- ###

# å°è¯•å¯¼å…¥ä¾èµ–
try:
    from clickhouse_driver import Client
except ImportError:
    Client = None

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None

BATCH_SIZE = 10000  # ä½ å¯ä»¥æ ¹æ®å†…å­˜è°ƒæ•´

# ... cleanup_myscale_db, coerce_value, translate_type_for_myscale, 
# ... translate_schema_for_myscale, find_database_files, sqlite_conn, 
# ... get_sqlite_schema, get_vec_info å‡½æ•°ä¿æŒä¸å˜ ...
# [æ­¤å¤„çœç•¥äº†æ‰€æœ‰æœªä¿®æ”¹çš„å‡½æ•°ï¼Œä»¥èŠ‚çœç©ºé—´]
# [æ‚¨åŸå§‹è„šæœ¬ä¸­çš„ cleanup_myscale_db ... get_vec_info å‡½æ•°åº”æ”¾åœ¨è¿™é‡Œ]
def cleanup_myscale_db(args, db_name):
    """è¿æ¥åˆ° MyScale/ClickHouse æœåŠ¡å™¨å¹¶åˆ é™¤ä¸€ä¸ªç‰¹å®šçš„æ•°æ®åº“ã€‚"""
    client = None
    try:
        logging.info("  ğŸ§¹ æ­£åœ¨æ¸…ç†å¤±è´¥çš„ MyScale æ•°æ®åº“ '%s'...", db_name)
        # è¿æ¥æ—¶ä¸éœ€è¦æŒ‡å®šæ•°æ®åº“
        client = Client(
            host=args.host,
            port=args.port,
            user=args.user,
            password=args.password,
            # secure=True # MyScale é€šå¸¸ä½¿ç”¨ SSL/TLS
        )
        client.execute(f'DROP DATABASE IF EXISTS `{db_name}`')
        logging.info("  âœ” æ¸…ç†æˆåŠŸã€‚")
    except Exception as e:
        logging.error("  ğŸ”´ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: %s", e)
        logging.error("     ä½ å¯èƒ½éœ€è¦æ‰‹åŠ¨åˆ é™¤æ•°æ®åº“ '%s'ã€‚", db_name)
    finally:
        if client:
            client.disconnect()

def coerce_value(value, target_type_str):
    """
    æ ¹æ®ç›®æ ‡æ•°æ®åº“çš„åˆ—ç±»å‹å­—ç¬¦ä¸²ï¼Œå°† Python å€¼å¼ºåˆ¶è½¬æ¢ä¸ºæ›´å…·ä½“çš„ç±»å‹ã€‚
    (ä»åŸè„šæœ¬ä¸­é‡ç”¨)
    """
    if value is None:
        return None

    target_type_str = target_type_str.lower()
    if 'nullable' in target_type_str:
        target_type_str = re.sub(r'nullable\((.+?)\)', r'\1', target_type_str)

    if any(int_type in target_type_str for int_type in ['int', 'bigint', 'smallint', 'tinyint']):
        try:
            return int(value)
        except (ValueError, TypeError):
            return None

    if ('array' not in target_type_str) and ('vector' not in target_type_str) and any(float_type in target_type_str for float_type in ['float', 'double', 'real', 'numeric', 'decimal']):
        try:
            return float(value)
        except (ValueError, TypeError):
            return None
    
    is_date_type = 'date' in target_type_str and 'datetime' not in target_type_str
    is_datetime_type = 'datetime' in target_type_str or 'timestamp' in target_type_str
    if isinstance(value, str) and (is_date_type or is_datetime_type):
        datetime_formats = [
            '%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%dT%H:%M:%S.%f', '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%d %H:%M', '%Y-%m-%d',
        ]
        for fmt in datetime_formats:
            try:
                return datetime.strptime(value, fmt)
            except (ValueError, TypeError):
                continue
        return None

    if 'bool' in target_type_str:
        if isinstance(value, int): return value != 0
        if isinstance(value, str):
            val_lower = value.lower()
            if val_lower in ('true', 't', '1', 'yes', 'y'): return True
            if val_lower in ('false', 'f', '0', 'no', 'n'): return False
        return bool(value)

    if any(str_type in target_type_str for str_type in ['string', 'text', 'char', 'clob']):
        if not isinstance(value, str):
            return str(value)
    return value

# --- Schema Translation Module ---

def translate_type_for_myscale(sqlite_type):
    """å°† SQLite æ•°æ®ç±»å‹æ˜ å°„åˆ° MyScale/ClickHouse ç±»å‹ã€‚(ä¸åŸè„šæœ¬ç›¸åŒ)"""
    sqlite_type = sqlite_type.upper()
    if 'INT' in sqlite_type: return 'Int64'
    if 'CHAR' in sqlite_type or 'TEXT' in sqlite_type or 'CLOB' in sqlite_type: return 'String'
    if 'BLOB' in sqlite_type: return 'String' # BLOB å°†è¢«è§£åŒ…ä¸º Array(Float32) æˆ–ä¿ç•™ä¸º String
    if 'REAL' in sqlite_type or 'FLOA' in sqlite_type or 'DOUB' in sqlite_type: return 'Float64'
    if 'NUMERIC' in sqlite_type or 'DECIMAL' in sqlite_type: return 'Decimal(38, 6)'
    if 'DATE' in sqlite_type: return 'Date'
    if 'DATETIME' in sqlite_type: return 'DateTime'
    return 'String'

def translate_schema_for_myscale(create_sql):
    """
    å°† SQLite CREATE TABLE è¯­å¥ (åŒ…æ‹¬ vec0 è™šæ‹Ÿè¡¨) è½¬æ¢ä¸º MyScale å…¼å®¹çš„æ ¼å¼ã€‚
    *** è¿™æ˜¯å…³é”®çš„ä¿®æ”¹ç‚¹ ***
    """
    create_sql = re.sub(r'--.*', '', create_sql)
    table_name_match = re.search(
        r'CREATE\s+(?:VIRTUAL\s+)?TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?\s*([`"\[]\S+[`"\]]|[^\s(]+)',
        create_sql, flags=re.IGNORECASE
    )
    if not table_name_match:
        raise ValueError(f"æ— æ³•ä» SQL ä¸­è§£æè¡¨å: {create_sql}")
    table_name = table_name_match.group(1).strip('`"[]\'')
    
    lines, constraints, indices = [], [], []
    
    columns_part_match = re.search(r'\((.*)\)', create_sql, re.DOTALL)
    if not columns_part_match:
        # å¯èƒ½æ˜¯ vec0ï¼Œä½¿ç”¨ USING è¯­æ³•
        columns_part_match = re.search(r'USING\s+vec0\s*\((.*)\)', create_sql, re.IGNORECASE | re.DOTALL)
        if not columns_part_match:
            raise ValueError(f"æ— æ³•ä» SQL ä¸­è§£æåˆ—å®šä¹‰: {create_sql}")
            
    columns_part = columns_part_match.group(1)
    columns_defs = re.split(r',(?![^\(]*\))', columns_part)
    
    for col_def in columns_defs:
        col_def = col_def.strip()
        if not col_def or col_def.upper().startswith(('PRIMARY KEY', 'FOREIGN KEY', 'UNIQUE', 'CONSTRAINT', 'CHECK')):
            continue
        
        # åŒ¹é…: `my_vector` float[384]
        vec_col_match = re.match(r'([`"\[]?\w+[`"\]]?)\s+float\s*\[\s*(\d+)\s*\]', col_def, re.IGNORECASE)
        if vec_col_match:
            col_name = vec_col_match.group(1).strip('`"')
            dimension = int(vec_col_match.group(2))
            
            # 1. æ·»åŠ åˆ—å®šä¹‰
            lines.append(f'`{col_name}` Array(Float32)')
            # 2. æ·»åŠ ç»´åº¦çº¦æŸ (MyScale/ClickHouse æ¨è)
            constraints.append(f'CONSTRAINT `{col_name}_dims` CHECK length(`{col_name}`) = {dimension}')
            # 3. æ·»åŠ  MyScale å‘é‡ç´¢å¼• (è¿™é‡Œä½¿ç”¨ MSTG ä½œä¸ºç¤ºä¾‹, ä½ å¯ä»¥æ¢æˆ HNSW ç­‰)
            indices.append(f'VECTOR INDEX `v_idx_{col_name}` `{col_name}` TYPE MSTG')
        else:
            parts = re.split(r'\s+', col_def, 2)
            col_name = parts[0].strip('`"')
            if not col_name:
                continue
            if len(parts) > 1 and parts[1]:
                sqlite_type = parts[1].split('(')[0]
            else:
                # æŸäº›è¡¨ (å¦‚ concept_vector) åªæœ‰åˆ—åæ²¡æœ‰ç±»å‹ï¼Œé»˜è®¤å½“ä½œ TEXT
                sqlite_type = 'TEXT'
            ch_type = translate_type_for_myscale(sqlite_type)
            if 'NOT NULL' not in col_def.upper():
                ch_type = f'Nullable({ch_type})'
            lines.append(f'`{col_name}` {ch_type}')
                
    all_definitions = lines + constraints + indices
    if not all_definitions:
        raise ValueError(f"æ— æ³•ç”Ÿæˆ `{table_name}` çš„åˆ—å®šä¹‰: {create_sql}")
    create_table_ch = f"CREATE TABLE `{table_name}` (\n    "
    create_table_ch += ',\n    '.join(all_definitions)
    # MyScale æ¨èä½¿ç”¨ MergeTree æˆ– ReplicatedMergeTree
    create_table_ch += f"\n) ENGINE = MergeTree() ORDER BY tuple();"
    return create_table_ch

# --- Database File Search ---

def find_database_files(source_path):
    """(ä»åŸè„šæœ¬ä¸­é‡ç”¨)"""
    database_files = []
    if os.path.isfile(source_path):
        candidates = [source_path]
    elif os.path.isdir(source_path):
        logging.info("ğŸ” æ­£åœ¨æœç´¢ç›®å½• '%s' ä¸­çš„æ•°æ®åº“æ–‡ä»¶...", source_path)
        patterns = ['**/*.db', '**/*.sqlite', '**/*.sqlite3']
        candidates = []
        for pattern in patterns:
            candidates.extend(glob.glob(os.path.join(source_path, pattern), recursive=True))
        candidates = sorted(set(candidates))
        logging.info("âœ” æ‰¾åˆ° %d ä¸ªæ•°æ®åº“æ–‡ä»¶ã€‚", len(candidates))
    else:
        logging.error("âœ– è·¯å¾„ '%s' ä¸å­˜åœ¨ã€‚", source_path)
        return []

    for path in candidates:
        if not path.lower().endswith(('.db', '.sqlite', '.sqlite3')):
            continue
        try:
            size = os.path.getsize(path)
        except OSError:
            continue
        if size == 0:
            logging.warning("âš ï¸ æ£€æµ‹åˆ°ç©ºçš„ SQLite æ–‡ä»¶ï¼Œå·²è·³è¿‡: %s", path)
            continue
        database_files.append(path)

    return database_files

# --- Database Connection & Operations ---

@contextmanager
def sqlite_conn(db_path):
    """(ä»åŸè„šæœ¬ä¸­é‡ç”¨)"""
    conn = sqlite3.connect(db_path)
    try:
        conn.enable_load_extension(True)
        import sqlite_vec
        sqlite_vec.load(conn)
        logging.info("âœ” sqlite-vec æ‰©å±•å·²åŠ è½½ã€‚")
    except Exception as e:
        logging.warning("ğŸŸ¡ åŠ è½½ sqlite-vec æ‰©å±•å¤±è´¥: %s", e)
    try:
        yield conn
    finally:
        conn.close()

def get_sqlite_schema(conn):
    """(ä»åŸè„šæœ¬ä¸­é‡ç”¨)"""
    cursor = conn.cursor()
    cursor.execute("SELECT name, sql FROM sqlite_master WHERE (type='table' OR type='view') AND name NOT LIKE 'sqlite_%' AND sql IS NOT NULL;")
    schema = cursor.fetchall()
    if not schema:
        raise RuntimeError("æº SQLite æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°è¡¨ã€‚")
    return schema

def get_vec_info(conn):
    """(ä»åŸè„šæœ¬ä¸­é‡ç”¨)"""
    vec_info = {}
    cursor = conn.cursor()
    cursor.execute("SELECT name, sql FROM sqlite_master WHERE sql LIKE '%USING vec0%';")
    vec_tables = cursor.fetchall()
    for table_name, create_sql in vec_tables:
        match = re.search(r'USING\s+vec0\s*\((.*)\)', create_sql, re.IGNORECASE | re.DOTALL)
        if not match:
            continue
        vec_info[table_name] = {'columns': []}
        defs_part = match.group(1).strip()
        columns_defs = re.split(r',(?![^\(]*\))', defs_part)
        for col_def in columns_defs:
            vec_col_match = re.match(r'([`"\[]?\w+[`"\]]?)\s+float\s*\[\s*(\d+)\s*\]', col_def.strip(), re.IGNORECASE)
            if vec_col_match:
                col_name = vec_col_match.group(1).strip('`"[]')
                dimension = int(vec_col_match.group(2))
                vec_info[table_name]['columns'].append({'name': col_name, 'dim': dimension})
    return vec_info


def migrate_to_myscale(args, db_name, source_db_path):
    """æ‰§è¡Œåˆ° MyScale çš„è¿ç§» (åŸºäº migrate_to_clickhouse)ã€‚"""
    if not Client:
        logging.error("clickhouse-driver æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install clickhouse-driver'ã€‚")
        return
    
    ### --- MODIFIED --- ###
    # åœ¨å¹¶è¡Œå·¥ä½œè¿›ç¨‹ä¸­ä¸æ˜¾ç¤º tqdm
    # if not tqdm:
    #     logging.warning("tqdm æœªå®‰è£…ã€‚è¿›åº¦æ¡å°†ä¸ä¼šæ˜¾ç¤ºã€‚")
    
    logging.info("\nğŸš€ å¼€å§‹è¿ç§»åˆ° MyScale...")
    client = None
    try:
        logging.info("è¿æ¥åˆ° MyScale (Host: %s) æ¥åˆ›å»ºæ•°æ®åº“...", args.host)
        # 1. è¿æ¥åˆ° admin æ•°æ®åº“ (é€šå¸¸æ˜¯ 'default' æˆ–ä¸æŒ‡å®š) ä»¥åˆ›å»ºæ–°æ•°æ®åº“
        with Client(
            host=args.host,
            port=args.port,
            user=args.user,
            password=args.password,
            # secure=True  # MyScale éœ€è¦å®‰å…¨è¿æ¥
        ) as admin_client:
            logging.info("æ­£åœ¨åˆ›å»ºæ•°æ®åº“ (å¦‚æœä¸å­˜åœ¨): %s", db_name)
            admin_client.execute(f'CREATE DATABASE IF NOT EXISTS `{db_name}`')
            logging.info("âœ” æ•°æ®åº“ '%s' å·²å‡†å¤‡å¥½ã€‚", db_name)

        # 2. è¿æ¥åˆ°æ–°åˆ›å»ºçš„æ•°æ®åº“
        logging.info("æ­£åœ¨è¿æ¥åˆ°æ–°æ•°æ®åº“ '%s'...", db_name)
        client = Client(
            host=args.host,
            port=args.port,
            user=args.user,
            password=args.password,
            database=db_name,
            # secure=True # ç¡®ä¿å®‰å…¨è¿æ¥
        )

        with sqlite_conn(source_db_path) as source_conn:
            vec_tables_info = get_vec_info(source_conn)
            if vec_tables_info:
                logging.info("âœ” æ£€æµ‹åˆ° %d ä¸ª sqlite-vec è¡¨: %s", len(vec_tables_info), ', '.join(vec_tables_info.keys()))

            source_cur = source_conn.cursor()
            tables_schema = get_sqlite_schema(source_conn)

            for table_name, create_sql in tables_schema:
                # è¿‡æ»¤æ‰ sqlite-vec çš„å†…éƒ¨åˆ†ç‰‡è¡¨
                if table_name == 'sqlite_sequence' or table_name.endswith(('_info', '_chunks', '_rowids')) or any(suffix in table_name for suffix in ('_metadatatext', '_metadatachunks', '_vector_chunks')):
                    continue
                
                logging.info("\n-- æ­£åœ¨å¤„ç†è¡¨: %s --", table_name)
                logging.info("  - è½¬æ¢ Schema...")
                myscale_create_sql = translate_schema_for_myscale(create_sql)
                
                logging.info("  - æ­£åœ¨ MyScale ä¸­åˆ›å»ºè¡¨ '%s'...", table_name)
                client.execute(f"DROP TABLE IF EXISTS `{table_name}`")
                client.execute(myscale_create_sql)
                
                logging.info("  - è·å– MyScale ç›®æ ‡è¡¨ç»“æ„...")
                target_column_types = {name: type_str for name, type_str in client.execute(f"SELECT name, type FROM system.columns WHERE database = '{db_name}' AND table = '{table_name}'")}
                
                logging.info("  - å‡†å¤‡ä» SQLite æå–æ•°æ®...")
                source_cur.execute(f'SELECT COUNT(*) FROM `{table_name.strip("`[]")}`')
                total_rows = source_cur.fetchone()[0]
                if total_rows == 0:
                    logging.info("  - è¡¨ '%s' ä¸ºç©º, è·³è¿‡æ•°æ®æ’å…¥ã€‚", table_name)
                    continue

                source_cur.execute(f'SELECT * FROM `{table_name.strip("`[]")}`')
                original_column_names = [desc[0] for desc in source_cur.description]
                column_names = list(original_column_names)

                try:
                    rowid_index = column_names.index('rowid')
                    del column_names[rowid_index]
                except ValueError:
                    rowid_index = -1
                
                vec_info_for_table = vec_tables_info.get(table_name)
                vector_column_indices = {}
                if vec_info_for_table:
                    logging.info("  - æ­£åœ¨è§£ç  sqlite-vec å‘é‡æ•°æ®...")
                    vector_column_indices = {i: item['dim'] for i, name in enumerate(column_names) for item in vec_info_for_table['columns'] if item['name'] == name}

                logging.info("  - å¼€å§‹å‘ MyScale æ‰¹é‡æ’å…¥ %d è¡Œæ•°æ®...", total_rows)
                
                ### --- MODIFIED --- ###
                # ç¦ç”¨å†…éƒ¨ tqdm è¿›åº¦æ¡
                # progress_bar = tqdm(total=total_rows, desc=f"  ğŸ“¤ è¿ç§» {table_name}", unit="rows") if tqdm else None
                progress_bar = None
                
                data_to_insert = []

                while True:
                    rows_batch = source_cur.fetchmany(BATCH_SIZE)
                    if not rows_batch: break
                    
                    processed_rows = [list(row) for row in rows_batch]
                    if rowid_index != -1:
                        original_rowid_index = original_column_names.index('rowid')
                        for row in processed_rows: del row[original_rowid_index]
                    
                    if vec_info_for_table:
                        for row in processed_rows:
                            for i, dim in vector_column_indices.items():
                                blob = row[i]
                                if isinstance(blob, bytes):
                                    num_floats = len(blob) // 4
                                    if num_floats == dim:
                                        row[i] = list(struct.unpack(f'<{dim}f', blob))
                                    else:
                                        # å¡«å……æˆ–æˆªæ–­ (ä¸åŸè„šæœ¬é€»è¾‘ç›¸åŒ)
                                        unpacked_data = list(struct.unpack(f'<{num_floats}f', blob))
                                        row[i] = (unpacked_data + [0.0] * dim)[:dim]

                    final_data_batch = []
                    for row in processed_rows:
                        coerced_row = [coerce_value(row[i], target_column_types.get(col_name, 'String')) for i, col_name in enumerate(column_names)]
                        final_data_batch.append(tuple(coerced_row))
                    
                    if not final_data_batch: continue
                    
                    data_to_insert.extend(final_data_batch)

                    # ä¸ºäº†æé«˜æ€§èƒ½ï¼Œå¯ä»¥ç´¯ç§¯æ›´å¤šæ•°æ®å†æ’å…¥ï¼Œä½†è¿™é‡Œæˆ‘ä»¬ä¿æŒåŸè„šæœ¬çš„é€»è¾‘
                    insert_statement = f"INSERT INTO `{table_name}` ({', '.join([f'`{c}`' for c in column_names])}) VALUES"
                    client.execute(insert_statement, data_to_insert, types_check=True)
                    
                    ### --- MODIFIED --- ###
                    # if progress_bar: progress_bar.update(len(data_to_insert))
                    
                    data_to_insert = [] # æ¸…ç©ºæ‰¹æ¬¡
                
                ### --- MODIFIED --- ###
                # if progress_bar: progress_bar.close()
                logging.info("  âœ” è¡¨ '%s' æ•°æ®è¿ç§»å®Œæˆã€‚", table_name)
        logging.info("\nğŸ‰ æ‰€æœ‰è¡¨å·²æˆåŠŸè¿ç§»åˆ° MyScaleï¼")

    except Exception as e:
        logging.error("âŒ è¿ç§»åˆ° MyScale å¤±è´¥: %s", e)
        # è§¦å‘æ¸…ç†
        cleanup_myscale_db(args, db_name)
        raise e # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥åœæ­¢è„šæœ¬
    finally:
        if client: client.disconnect()

### --- NEW --- ###
def run_myscale_migration_task(db_file, args):
    """
    ä¸€ä¸ªç‹¬ç«‹çš„å·¥ä½œå‡½æ•°ï¼Œç”¨äºåœ¨è¿›ç¨‹æ± ä¸­è¿è¡Œã€‚
    å¤„ç†å•ä¸ª SQLite æ–‡ä»¶åˆ° MyScale çš„è¿ç§»ã€‚
    """
    db_name = os.path.splitext(os.path.basename(db_file))[0]
    db_name = re.sub(r'[^a-zA-Z0-9_]', '_', str(db_name)).lower()
    
    logging.info("======================================================================")
    logging.info("ğŸ”„ [Worker] æ­£åœ¨å¤„ç†: %s (ç›®æ ‡æ•°æ®åº“: %s)", os.path.basename(db_file), db_name)
    logging.info("======================================================================")
    
    try:
        # è°ƒç”¨æ ¸å¿ƒè¿ç§»é€»è¾‘
        migrate_to_myscale(args, db_name, db_file)
        logging.info("âœ… [Worker] æˆåŠŸ: %s", db_name)
        return (db_name, True, None) # (æ•°æ®åº“åç§°, æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
    except Exception as e:
        # è®°å½•é”™è¯¯ï¼Œä½†å…è®¸ä¸»è¿›ç¨‹ç»§ç»­
        logging.error("âŒ [Worker] å¤±è´¥: %s. é”™è¯¯: %s", db_name, e)
        return (db_name, False, str(e))
### --- NEW --- ###


def main():
    logging.basicConfig(
        level=logging.INFO, 
        ### --- MODIFIED --- ###
        # æ·»åŠ  %(processName)s ä»¥åŒºåˆ†æ¥è‡ªä¸åŒè¿›ç¨‹çš„æ—¥å¿—
        format='%(asctime)s - %(levelname)s - [%(processName)s] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    if Client is None:
        logging.error("clickhouse-driver æœªæ‰¾åˆ°ã€‚è¯·è¿è¡Œ 'pip install clickhouse-driver'ã€‚")
        logging.warning("MyScale è¿ç§»å°†ä¸å¯ç”¨ã€‚")
    if tqdm is None:
        logging.info("tqdm æœªæ‰¾åˆ°ã€‚è¿›åº¦æ¡å°†ä¸ä¼šæ˜¾ç¤ºã€‚")

    parser = argparse.ArgumentParser(
        description="å°† SQLite æ•°æ®åº“ (åŒ…æ‹¬ sqlite-vec) è¿ç§»åˆ° MyScaleã€‚",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument('--source', default='/mnt/DataFlow/ydw/Text2VectorSQL/Data_Synthesizer/pipeline/sqlite/results/spider/vector_databases', help="æº SQLite æ•°æ®åº“æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹è·¯å¾„ã€‚")
    parser.add_argument('--host', default='112.126.57.89', help="[MyScale] é›†ç¾¤ä¸»æœºå (ä¾‹å¦‚ 'your-cluster.db.myscale.com')ã€‚")
    parser.add_argument('--port', type=int, default=9000, help="[MyScale] å®‰å…¨è¿æ¥ç«¯å£ (é€šå¸¸æ˜¯ 8443)ã€‚")
    parser.add_argument('--user', default='default', help="[MyScale] ç”¨æˆ·åã€‚")
    parser.add_argument('--password', default='myscale#EDC', help="[MyScale] å¯†ç ã€‚")
    
    ### --- NEW --- ###
    # æ·»åŠ  --workers å‚æ•°
    # é»˜è®¤å€¼ï¼šmin(32, cpu_count + 4)ã€‚å¯¹äºæœ‰ 100+ CPU çš„ä½ ï¼Œå¯ä»¥è®¾ç½®ä¸€ä¸ªæ›´é«˜çš„é»˜è®¤å€¼ï¼Œ
    # ä½† 32 æ˜¯ä¸€ä¸ªå®‰å…¨ï¼ˆä¸”é€šå¸¸é«˜æ•ˆï¼‰çš„èµ·ç‚¹ï¼Œä»¥é¿å…ä½¿æ•°æ®åº“è¿‡è½½ã€‚
    try:
        default_workers = min(32, (multiprocessing.cpu_count() or 1) + 4)
    except NotImplementedError:
        default_workers = 8 # ä¸€ä¸ªä¿å®ˆçš„å¤‡ç”¨å€¼
        
    parser.add_argument('--workers', type=int, default=default_workers, help="è¦è¿è¡Œçš„å¹¶è¡Œè¿ç§»è¿›ç¨‹æ•°ã€‚")
    ### --- NEW --- ###
    
    args = parser.parse_args()

    if not os.path.exists(args.source):
        logging.error("âœ– æºè·¯å¾„ '%s' ä¸å­˜åœ¨ã€‚", args.source)
        return

    database_files = find_database_files(args.source)
    if not database_files:
        logging.warning("âœ– æœªæ‰¾åˆ°å¯è¿ç§»çš„æ•°æ®åº“æ–‡ä»¶ã€‚")
        return

    ### --- MODIFIED --- ###
    # ç”¨å¹¶è¡Œæ‰§è¡Œå™¨æ›¿æ¢ä¸²è¡Œå¾ªç¯
    
    logging.info("\nğŸ“Š è¿ç§»ä»»åŠ¡:")
    logging.info("  æºè·¯å¾„: %s", args.source)
    logging.info("  ç›®æ ‡ç±»å‹: MyScale")
    logging.info("  ç›®æ ‡ä¸»æœº: %s:%s", args.host, args.port)
    logging.info("  æ‰¾åˆ°çš„æ•°æ®åº“æ–‡ä»¶: %d", len(database_files))
    logging.info("  å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°: %d", args.workers)

    success_dbs = []
    failed_dbs = []

    logging.info("ğŸš€ æ­£åœ¨å¯åŠ¨ %d ä¸ªå·¥ä½œè¿›ç¨‹çš„å¹¶è¡Œè¿ç§»...", args.workers)
    
    with ProcessPoolExecutor(max_workers=args.workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {
            executor.submit(run_myscale_migration_task, db_file, args): db_file
            for db_file in database_files
        }

        # è®¾ç½®ä¸»è¿›åº¦æ¡ (è·Ÿè¸ªæ–‡ä»¶)
        progress_bar = None
        if tqdm:
            progress_bar = tqdm(total=len(futures), desc="è¿ç§»æ•°æ®åº“æ–‡ä»¶", unit="db")

        # åœ¨ä»»åŠ¡å®Œæˆæ—¶å¤„ç†ç»“æœ
        for future in as_completed(futures):
            try:
                (db_name, success, error_msg) = future.result()
                if success:
                    success_dbs.append(db_name)
                else:
                    failed_dbs.append((db_name, error_msg))
            except Exception as e:
                # æ•è·å·¥ä½œè¿›ç¨‹æœ¬èº«çš„å´©æºƒ
                db_file_path = futures[future]
                db_name = os.path.splitext(os.path.basename(db_file_path))[0]
                logging.error("å·¥ä½œè¿›ç¨‹ %s æ„å¤–å´©æºƒ: %s", db_name, e)
                failed_dbs.append((db_name, str(e)))
            
            if progress_bar:
                progress_bar.update(1)

        if progress_bar:
            progress_bar.close()

    # --- æ–°çš„æœ€ç»ˆæŠ¥å‘Š ---
    success_count = len(success_dbs)
    error_count = len(failed_dbs)
    total_files = len(database_files)

    logging.info("\n======================================================================")
    logging.info("ğŸ¯ è¿ç§»å®Œæˆ (æ¨¡å¼: MyScale):")
    logging.info("   æ€»æ–‡ä»¶æ•°: %d", total_files)
    logging.info("   æˆåŠŸ: %d", success_count)
    logging.info("   å¤±è´¥: %d", error_count)
    logging.info("======================================================================")

    if error_count > 0:
        logging.warning("âš ï¸  %d ä¸ªæ•°æ®åº“è¿ç§»ä»»åŠ¡å¤±è´¥:", error_count)
        for db_name, error_msg in failed_dbs:
            logging.warning("    - %s (é”™è¯¯: %s)", db_name, error_msg)
    else:
        logging.info("ğŸ‰ æ‰€æœ‰æ•°æ®åº“è¿ç§»ä»»åŠ¡å‡å·²æˆåŠŸå®Œæˆï¼")


if __name__ == '__main__':
    main()
