import json
import random
import os
import sys

def load_json_file(filepath):
    """å¸¸è§„åŠ è½½.jsonæ–‡ä»¶ï¼ˆé€‚ç”¨äºå°æ–‡ä»¶ï¼‰ã€‚"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ° -> {filepath}ã€‚")
        sys.exit(1)
    except json.JSONDecodeError:
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ '{filepath}' ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚")
        sys.exit(1)

def count_lines_in_file(filepath):
    """å¿«é€Ÿè®¡ç®—æ–‡ä»¶è¡Œæ•°ï¼Œæ— éœ€åŠ è½½åˆ°å†…å­˜ã€‚"""
    print(f"   -> æ­£åœ¨å¿«é€Ÿè®¡ç®— '{os.path.basename(filepath)}' çš„æ€»è¡Œæ•°...")
    count = 0
    with open(filepath, 'r', encoding='utf-8') as f:
        for _ in f:
            count += 1
    return count

def reservoir_sample_jsonl(filepath, k):
    """
    ä½¿ç”¨è“„æ°´æ± æŠ½æ ·ä»ä¸€ä¸ªå¤§çš„ .jsonl æ–‡ä»¶ä¸­é«˜æ•ˆåœ°éšæœºæŠ½å– k ä¸ªæ ·æœ¬ã€‚
    è¿™åªä¼šåœ¨å†…å­˜ä¸­ä¿ç•™ k ä¸ªå…ƒç´ ã€‚
    """
    print(f"   -> æ­£åœ¨ä» '{os.path.basename(filepath)}' ä¸­è¿›è¡Œè“„æ°´æ± æŠ½æ ·...")
    reservoir = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i < k:
                # 1. ç›´æ¥å¡«æ»¡è“„æ°´æ± 
                reservoir.append(json.loads(line))
            else:
                # 2. ä»¥ k/i çš„æ¦‚ç‡æ›¿æ¢æ—§å…ƒç´ 
                j = random.randint(0, i)
                if j < k:
                    reservoir[j] = json.loads(line)
    return reservoir

def process_and_mix_datasets(file1_path, file2_path, output_dir, ratios):
    """
    æ ¹æ®ç»™å®šçš„æ¯”ä¾‹ï¼Œé«˜æ•ˆæ··åˆä¸¤ä¸ªJSONæ–‡ä»¶çš„æ•°æ®ã€‚
    """
    # --- 1. è¯†åˆ«æ–‡ä»¶ç±»å‹å¹¶è·å–å¤§å° ---
    # æˆ‘ä»¬å‡è®¾ .jsonl æ–‡ä»¶æ˜¯æ½œåœ¨çš„å¤§æ–‡ä»¶ï¼Œå¦ä¸€ä¸ªæ˜¯å°æ–‡ä»¶
    if file1_path.endswith('.jsonl') and file2_path.endswith('.json'):
        large_file_path, small_file_path = file1_path, file2_path
        large_file_is_file1 = True
    elif file2_path.endswith('.jsonl') and file1_path.endswith('.json'):
        large_file_path, small_file_path = file2_path, file1_path
        large_file_is_file1 = False
    else:
        print("âŒ é”™è¯¯: è„šæœ¬éœ€è¦ä¸€ä¸ª .json æ–‡ä»¶å’Œä¸€ä¸ª .jsonl æ–‡ä»¶æ‰èƒ½è¿›è¡Œä¼˜åŒ–ã€‚")
        print(f"   æ–‡ä»¶1: {file1_path}")
        print(f"   æ–‡ä»¶2: {file2_path}")
        sys.exit(1)

    print(f"è¯†åˆ«åˆ°å°æ–‡ä»¶ (å®Œå…¨åŠ è½½): {os.path.basename(small_file_path)}")
    print(f"è¯†åˆ«åˆ°å¤§æ–‡ä»¶ (æµå¼å¤„ç†): {os.path.basename(large_file_path)}")

    # åŠ è½½å°æ–‡ä»¶æ•°æ®ï¼Œå¹¶è·å–å¤§æ–‡ä»¶çš„è¡Œæ•°
    small_data = load_json_file(small_file_path)
    large_file_len = count_lines_in_file(large_file_path)

    print(f" -> å°æ–‡ä»¶åŠ è½½äº† {len(small_data)} æ¡æ•°æ®ã€‚")
    print(f" -> å¤§æ–‡ä»¶å…±æœ‰ {large_file_len} æ¡æ•°æ®ã€‚")

    # --- 2. ç¡®å®šåŸºå‡†æ•°é‡ ---
    # é€»è¾‘ä¿æŒä¸å˜ï¼šåŸºå‡†æ•°é‡ç”±ä¸¤ä¸ªæ–‡ä»¶ä¸­è¾ƒå°çš„é‚£ä¸€ä¸ªå†³å®š
    if len(small_data) <= large_file_len:
        base_size = len(small_data)
        base_is_from_small_file = True
        print(f"\næŠ½æ ·åŸºå‡†ç”±å°æ–‡ä»¶å†³å®šï¼Œæ•°é‡ = {base_size}")
    else:
        base_size = large_file_len
        base_is_from_small_file = False
        print(f"\næŠ½æ ·åŸºå‡†ç”±å¤§æ–‡ä»¶å†³å®šï¼Œæ•°é‡ = {base_size}")

    if base_size == 0:
        print("âŒ é”™è¯¯: åŸºå‡†æ–‡ä»¶ä¸­æ²¡æœ‰æ•°æ®ï¼Œæ— æ³•è¿›è¡Œæ··åˆã€‚")
        sys.exit(1)

    # --- 3. åˆ›å»ºè¾“å‡ºç›®å½• (æ‚¨çš„ä»£ç æ˜¯æ­£ç¡®çš„) ---
    os.makedirs(output_dir, exist_ok=True)
    print(f"ç»“æœå°†ä¿å­˜åˆ°ç›®å½•: '{output_dir}'")

    # --- 4. éå†æ¯”ä¾‹è¿›è¡Œå¤„ç† ---
    for ratio_str in ratios:
        # (è§£ææ¯”ä¾‹å­—ç¬¦ä¸²çš„ä»£ç ä¸æ‚¨çš„ä¸€æ ·ï¼Œæ‰€ä»¥è¿™é‡Œçœç•¥äº†é‡å¤éƒ¨åˆ†)
        r1_str, r2_str = ratio_str.split(':')
        r1, r2 = int(r1_str), int(r2_str)
        print(f"\n--- å¼€å§‹å¤„ç†æ¯”ä¾‹ {r1}:{r2} ---")
        
        # ç¡®å®šr_small, r_largeçš„å€¼
        if base_is_from_small_file:
            # æ ¹æ®å“ªä¸ªæ–‡ä»¶æ˜¯file1/file2æ¥åˆ†é…æ¯”ä¾‹å€¼
            r_small = r2 if large_file_is_file1 else r1
            r_large = r1 if large_file_is_file1 else r2
        else:
            r_small = r1 if large_file_is_file1 else r2
            r_large = r2 if large_file_is_file1 else r1

        # è®¡ç®—æŠ½æ ·æ•°é‡
        if r_small == 0 and r_large > 0:
            n_base = 0; n_other = base_size
        elif r_large == 0 and r_small > 0:
            n_base = base_size; n_other = 0
        elif r_small > 0 and r_large > 0:
            n_base = base_size
            n_other = int(base_size * (r_large / r_small))
        else:
            n_base = 0; n_other = 0

        # å°†æŠ½æ ·æ•°æ˜ å°„å› small_data å’Œ large_file
        n_small, n_large = (n_base, n_other) if base_is_from_small_file else (n_other, n_base)

        # é˜²æ­¢æŠ½æ ·æ•°è¶…è¿‡å®é™…æ•°æ®é‡
        n_small = min(n_small, len(small_data))
        n_large = min(n_large, large_file_len)

        print(f"è®¡åˆ’æŠ½æ ·: {n_small}æ¡ from '{os.path.basename(small_file_path)}', {n_large}æ¡ from '{os.path.basename(large_file_path)}'")

        # è¿›è¡ŒæŠ½æ ·
        sample_small = random.sample(small_data, n_small)
        sample_large = reservoir_sample_jsonl(large_file_path, n_large)

        # åˆå¹¶å¹¶æ‰“ä¹±
        combined_data = sample_small + sample_large
        random.shuffle(combined_data)
        print(f"æ··åˆåæ€»æ•°æ®é‡: {len(combined_data)}")

        # ç¡®å®šå“ªä¸ªæ˜¯file1çš„æ ·æœ¬
        sample_f1, sample_f2 = (sample_large, sample_small) if large_file_is_file1 else (sample_small, sample_large)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_filename = f"input_llm_{r1}_{r2}.json"
        output_path = os.path.join(output_dir, output_filename)
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(combined_data, f, indent=2, ensure_ascii=False)
            print(f"âœ… æˆåŠŸä¿å­˜åˆ°: {output_path}")
        except IOError as e:
            print(f"âŒ é”™è¯¯: æ— æ³•å†™å…¥æ–‡ä»¶ {output_path}ã€‚é”™è¯¯ä¿¡æ¯: {e}")

    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼")


if __name__ == "__main__":
    # --- è¯·åœ¨è¿™é‡Œé…ç½® ---
    
    # 1. è¾“å…¥æ–‡ä»¶è·¯å¾„
    # è„šæœ¬ä¼šè‡ªåŠ¨è¯†åˆ«å“ªä¸ªæ˜¯.jsonï¼Œå“ªä¸ªæ˜¯.jsonl
    FILE_1_PATH = "/mnt/b_public/data/ydw/Text2VectorSQL/Data_Synthesizer/pipeline/sqlite/results/synthesis_data_deversity/input_llm.json"
    FILE_2_PATH = "/mnt/b_public/data/ydw/datasets/input_llm.jsonl"

    # 2. è¾“å‡ºç›®å½•
    OUTPUT_DIR = "./results/mixed_datasets"

    # 3. éœ€è¦ç”Ÿæˆçš„æ··åˆæ¯”ä¾‹åˆ—è¡¨
    RATIOS_TO_GENERATE = [
        "1:0", "0:1", "1:1", "1:2", "2:1", "1:4", "4:1"
    ]
    
    # --- é…ç½®ç»“æŸï¼Œè¿è¡Œè„šæœ¬ ---
    process_and_mix_datasets(FILE_1_PATH, FILE_2_PATH, OUTPUT_DIR, RATIOS_TO_GENERATE)
