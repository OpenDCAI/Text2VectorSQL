import argparse
import os
import sqlite3
import re
import struct
import glob
from contextlib import contextmanager
from datetime import datetime

# å°è¯•å¯¼å…¥ä¾èµ–ï¼Œå¦‚æœå¤±è´¥åˆ™ç»™å‡ºæç¤º
try:
    import psycopg2
    from psycopg2.extras import execute_batch
except ImportError:
    psycopg2 = None
    execute_batch = None

try:
    from clickhouse_driver import Client
except ImportError:
    Client = None

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None

BATCH_SIZE = 10000 # You can adjust this value based on your available memory

def cleanup_postgres_db(args, db_name):
    """Connects to the PostgreSQL server and drops a specific database."""
    conn_admin = None
    try:
        print(f"  ğŸ§¹ Cleaning up failed PostgreSQL database '{db_name}'...")
        conn_admin = psycopg2.connect(
            host=args.host, port=args.port, user=args.user, password=args.password, dbname='postgres'
        )
        conn_admin.autocommit = True
        with conn_admin.cursor() as cur:
            # WITH (FORCE) is available from PostgreSQL 13+ and is very useful
            # for terminating any lingering connections from the failed script.
            cur.execute(f'DROP DATABASE IF EXISTS "{db_name}" WITH (FORCE);')
        print(f"  âœ” Cleanup successful.")
    except Exception as e:
        print(f"  ğŸ”´ Error during cleanup: {e}")
        print(f"     You may need to manually drop the database '{db_name}'.")
    finally:
        if conn_admin:
            conn_admin.close()

def cleanup_clickhouse_db(args, db_name):
    """Connects to the ClickHouse server and drops a specific database."""
    client = None
    try:
        print(f"  ğŸ§¹ Cleaning up failed ClickHouse database '{db_name}'...")
        client = Client(host=args.host, port=args.port, user=args.user, password=args.password)
        client.execute(f'DROP DATABASE IF EXISTS `{db_name}`')
        print(f"  âœ” Cleanup successful.")
    except Exception as e:
        print(f"  ğŸ”´ Error during cleanup: {e}")
        print(f"     You may need to manually drop the database '{db_name}'.")
    finally:
        if client:
            client.disconnect()

def coerce_value(value, target_type_str):
    """
    æ ¹æ®ç›®æ ‡æ•°æ®åº“çš„åˆ—ç±»å‹å­—ç¬¦ä¸²ï¼Œå°† Python å€¼å¼ºåˆ¶è½¬æ¢ä¸ºæ›´å…·ä½“çš„ç±»å‹ï¼Œ
    ä½¿è½¬æ¢è¿‡ç¨‹æ›´åŠ é²æ£’ã€‚
    """
    if value is None:
        return None

    target_type_str = target_type_str.lower()
    if 'nullable' in target_type_str:
        target_type_str = re.sub(r'nullable\((.+?)\)', r'\1', target_type_str)

    # --- æ•´æ•°è½¬æ¢ ---
    if any(int_type in target_type_str for int_type in ['int', 'bigint', 'smallint', 'tinyint']):
        try:
            return int(value)
        except (ValueError, TypeError):
            # print("  ğŸŸ¡ è­¦å‘Š: æ— æ³•å°† '{}' (ç±»å‹: {}) è½¬æ¢ä¸ºæ•´æ•°ã€‚å°†æ’å…¥ NULLã€‚".format(value, type(value).__name__))
            return None

    # --- æµ®ç‚¹æ•°/æ•°å€¼è½¬æ¢ ---
    if ('array' not in target_type_str) and ('vector' not in target_type_str) and any(float_type in target_type_str for float_type in ['float', 'double', 'real', 'numeric', 'decimal']):
        try:
            return float(value)
        except (ValueError, TypeError):
            print(target_type_str)
            # print("  ğŸŸ¡ è­¦å‘Š: æ— æ³•å°† '{}' (ç±»å‹: {}) è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚å°†æ’å…¥ NULLã€‚".format(value, type(value).__name__))
            return None
    
    # --- æ—¥æœŸ/æ—¶é—´è½¬æ¢ ---
    is_date_type = 'date' in target_type_str and 'datetime' not in target_type_str
    is_datetime_type = 'datetime' in target_type_str or 'timestamp' in target_type_str
    if isinstance(value, str) and (is_date_type or is_datetime_type):
        datetime_formats = [
            '%Y-%m-%d %H:%M:%S.%f',  # å¸¦æ¯«ç§’
            '%Y-%m-%d %H:%M:%S',    # å¸¦ç§’
            '%Y-%m-%dT%H:%M:%S.%f', # ISO 8601 æ ¼å¼
            '%Y-%m-%dT%H:%M:%S',   # ISO 8601 æ ¼å¼
            '%Y-%m-%d %H:%M',       # ä¸å¸¦ç§’
            '%Y-%m-%d',
        ]
        
        for fmt in datetime_formats:
            try:
                return datetime.strptime(value, fmt)
            except (ValueError, TypeError):
                continue # If this format fails, try the next one

        # print("  ğŸŸ¡ è­¦å‘Š: æ— æ³•å°†å­—ç¬¦ä¸² '{}' è§£æä¸ºä»»ä½•å·²çŸ¥çš„æ—¥æœŸ/æ—¶é—´æ ¼å¼ã€‚å°†æ’å…¥ NULLã€‚".format(value))
        return None

    # --- å¸ƒå°”è½¬æ¢ ---
    if 'bool' in target_type_str:
        if isinstance(value, int):
            return value != 0
        if isinstance(value, str):
            val_lower = value.lower()
            if val_lower in ('true', 't', '1', 'yes', 'y'):
                return True
            if val_lower in ('false', 'f', '0', 'no', 'n'):
                return False
        return bool(value)

    # --- å­—ç¬¦ä¸²å›é€€ ---
    if any(str_type in target_type_str for str_type in ['string', 'text', 'char', 'clob']):
        if not isinstance(value, str):
            return str(value)

    # å¦‚æœæ²¡æœ‰åº”ç”¨ç‰¹å®šçš„è½¬æ¢ï¼Œè¿”å›åŸå§‹å€¼
    return value

# --- Schema ç¿»è¯‘æ¨¡å— ---

def translate_type_for_postgres(sqlite_type):
    """å°† SQLite æ•°æ®ç±»å‹æ˜ å°„åˆ° PostgreSQL ç±»å‹"""
    sqlite_type = sqlite_type.upper()
    if 'INT' in sqlite_type:
        return 'BIGINT'
    if 'CHAR' in sqlite_type or 'TEXT' in sqlite_type or 'CLOB' in sqlite_type:
        return 'TEXT'
    if 'BLOB' in sqlite_type:
        return 'BYTEA'
    if 'REAL' in sqlite_type or 'FLOA' in sqlite_type or 'DOUB' in sqlite_type:
        return 'DOUBLE PRECISION'
    if 'NUMERIC' in sqlite_type or 'DECIMAL' in sqlite_type:
        return 'NUMERIC'
    return 'TEXT' # é»˜è®¤å›é€€

def translate_schema_for_postgres(create_sql):
    """å°† SQLite çš„ CREATE TABLE è¯­å¥ï¼ˆåŒ…æ‹¬ vec0 è™šæ‹Ÿè¡¨ï¼‰ç¿»è¯‘ä¸º PostgreSQL å…¼å®¹æ ¼å¼"""
    # æ£€æŸ¥æ˜¯å¦ä¸º sqlite-vec è™šæ‹Ÿè¡¨
    is_virtual_vec_table = 'USING VEC0' in create_sql.upper() and 'VIRTUAL' in create_sql.upper()

    # ç»Ÿä¸€å¤„ç†å¼•å·
    create_sql = create_sql.replace('`', '"')

    if is_virtual_vec_table:
        # ä» "CREATE VIRTUAL TABLE" ä¸­æå–è¡¨å
        table_name_match = re.search(
            r'CREATE\s+VIRTUAL\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?\s*([`"\[]\S+[`"\]]|\S+)',
            create_sql, flags=re.IGNORECASE
        )
        if not table_name_match:
            raise ValueError(f"æ— æ³•ä» vec0 SQL ä¸­è§£æè¡¨å: {create_sql}")
        table_name = table_name_match.group(1)

        # ä» "USING vec0(...)" ä¸­æå–åˆ—å®šä¹‰
        columns_part_match = re.search(r'USING\s+vec0\s*\((.*)\)', create_sql, re.IGNORECASE | re.DOTALL)
        if not columns_part_match:
            raise ValueError(f"æ— æ³•ä» vec0 SQL ä¸­è§£æåˆ—å®šä¹‰: {create_sql}")
        columns_part = columns_part_match.group(1)

        columns_defs = re.split(r',(?![^\(]*\))', columns_part)
        new_defs = []
        for col_def in columns_defs:
            col_def = col_def.strip()
            if not col_def:
                continue

            # åŒ¹é…å‘é‡åˆ—ï¼Œä¾‹å¦‚ "col_embedding float[384]"
            vec_col_match = re.match(r'([`"\[]?\w+[`"\]]?)\s+float\s*\[\s*(\d+)\s*\]', col_def, re.IGNORECASE)
            if vec_col_match:
                col_name = vec_col_match.group(1).replace('`', '"')
                if '"' not in col_name:
                    col_name = f'"{col_name}"'
                dimension = int(vec_col_match.group(2))
                # ç¿»è¯‘ä¸º pgvector ç±»å‹
                new_defs.append(f'{col_name} vector({dimension})')
            else:
                # å¤„ç†è™šæ‹Ÿè¡¨å®šä¹‰ä¸­çš„æ™®é€šåˆ—
                col_match = re.match(r'([`"\[]?\w+[`"\]]?)\s+([A-Z]+(?:\(\d+(?:,\d+)?\))?)(.*)', col_def, flags=re.IGNORECASE)
                if col_match:
                    col_name, col_type, constraints = col_match.groups()
                    col_name = col_name.replace('`', '"')
                    if '"' not in col_name:
                        col_name = f'"{col_name}"'
                    pg_type = translate_type_for_postgres(col_type)
                    new_defs.append(f'{col_name} {pg_type}{constraints}')
                else:
                    new_defs.append(col_def) # å›é€€

        # æ„å»ºæ ‡å‡†çš„ CREATE TABLE è¯­å¥
        return f"CREATE TABLE {table_name} ({', '.join(new_defs)})"

    # --- å¯¹æ ‡å‡†è¡¨çš„ç°æœ‰å¤„ç†é€»è¾‘ ---
    create_sql = re.sub(r'\bAUTOINCREMENT\b', '', create_sql, flags=re.IGNORECASE)
    create_sql = re.sub(r'INTEGER\s+PRIMARY\s+KEY', 'INTEGER', create_sql, flags=re.IGNORECASE)

    try:
        table_name_match = re.search(r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?\s*([`"\[]\S+[`"\]]|\S+)\s*(\(.*\))', create_sql, flags=re.IGNORECASE | re.DOTALL)
        if not table_name_match:
            return create_sql
            
        table_name = table_name_match.group(1)
        defs_part = table_name_match.group(2)
        
        defs_content = defs_part.strip()[1:-1]
        
        defs_list = re.split(r',(?![^\(]*\))', defs_content)
        new_defs = []
        for definition in defs_list:
            definition = definition.strip()
            
            if definition.upper().startswith(('PRIMARY KEY', 'FOREIGN KEY', 'UNIQUE', 'CONSTRAINT', 'CHECK')):
                # new_defs.append(definition)
                continue

            col_match = re.match(r'([`"\[]?\w+[`"\]]?)\s+([A-Z]+(?:\(\d+(?:,\d+)?\))?)(.*)', definition, flags=re.IGNORECASE)
            if col_match:
                col_name, col_type, constraints = col_match.groups()
                if '"' not in col_name:
                    col_name = f'"{col_name}"'
                pg_type = translate_type_for_postgres(col_type)
                new_defs.append(f'{col_name} {pg_type}{constraints}')
            else:
                new_defs.append(definition)

        return f"CREATE TABLE {table_name} ({', '.join(new_defs)})"
    except Exception:
        return re.sub(r'\bINTEGER\b', 'BIGINT', create_sql, flags=re.IGNORECASE)


def translate_type_for_clickhouse(sqlite_type):
    """å°† SQLite æ•°æ®ç±»å‹æ˜ å°„åˆ° ClickHouse ç±»å‹"""
    sqlite_type = sqlite_type.upper()
    if 'INT' in sqlite_type: return 'Int64'
    if 'CHAR' in sqlite_type or 'TEXT' in sqlite_type or 'CLOB' in sqlite_type: return 'String'
    if 'BLOB' in sqlite_type: return 'String' # é»˜è®¤ä¸º Stringï¼Œå‘é‡ BLOB ä¼šè¢«ç‰¹æ®Šå¤„ç†
    if 'REAL' in sqlite_type or 'FLOA' in sqlite_type or 'DOUB' in sqlite_type: return 'Float64'
    if 'NUMERIC' in sqlite_type or 'DECIMAL' in sqlite_type: return 'Decimal(38, 6)'
    if 'DATE' in sqlite_type: return 'Date'
    if 'DATETIME' in sqlite_type: return 'DateTime'
    return 'String'

def translate_schema_for_clickhouse(create_sql):
    """å°† SQLite çš„ CREATE TABLE è¯­å¥ï¼ˆåŒ…æ‹¬ vec0 è™šæ‹Ÿè¡¨ï¼‰ç¿»è¯‘ä¸º ClickHouse å…¼å®¹æ ¼å¼"""
    create_sql = re.sub(r'--.*', '', create_sql)

    table_name_match = re.search(
        # r'CREATE\s+(?:VIRTUAL\s+)?TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?\s*([`"\[]\S+[`"\]]|\S+)',
        r'CREATE\s+(?:VIRTUAL\s+)?TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?\s*([`"\[]\S+[`"\]]|[^\s(]+)',
        create_sql, flags=re.IGNORECASE
    )
    if not table_name_match:
        raise ValueError(f"æ— æ³•ä» SQL ä¸­è§£æè¡¨å: {create_sql}")
        
    table_name = table_name_match.group(1).strip('`"[]\'')

    lines = []
    indices = []

    columns_part_match = re.search(r'\((.*)\)', create_sql, re.DOTALL)
    if not columns_part_match:
        raise ValueError(f"æ— æ³•ä» SQL ä¸­è§£æåˆ—å®šä¹‰: {create_sql}")
    columns_part = columns_part_match.group(1)
    
    columns_defs = re.split(r',(?![^\(]*\))', columns_part)

    for col_def in columns_defs:
        col_def = col_def.strip()
        if not col_def or col_def.upper().startswith(('PRIMARY KEY', 'FOREIGN KEY', 'UNIQUE', 'CONSTRAINT', 'CHECK')):
            continue

        vec_col_match = re.match(r'([`"\[]?\w+[`"\]]?)\s+float\s*\[\s*(\d+)\s*\]', col_def, re.IGNORECASE)
        if vec_col_match:
            col_name = vec_col_match.group(1).strip('`"')
            dimension = int(vec_col_match.group(2))
            
            lines.append(f'`{col_name}` Array(Float32)')
            
            indices.append(
                f"INDEX `idx_vec_{col_name}` `{col_name}` TYPE vector_similarity('hnsw', 'L2Distance', {dimension}) GRANULARITY 1000"
            )
        else:
            parts = re.split(r'\s+', col_def, 2)
            col_name = parts[0].strip('`"')
            
            if len(parts) > 1:
                sqlite_type = parts[1].split('(')[0]
                ch_type = translate_type_for_clickhouse(sqlite_type)
                if 'NOT NULL' not in col_def.upper():
                    ch_type = f'Nullable({ch_type})'
                lines.append(f'`{col_name}` {ch_type}')

    create_table_ch = f"CREATE TABLE `{table_name}` (\n    "
    all_definitions = lines + indices
    create_table_ch += ',\n    '.join(all_definitions)
    create_table_ch += f"\n) ENGINE = MergeTree() ORDER BY tuple();"
    
    return create_table_ch


# --- æ•°æ®åº“æ–‡ä»¶æœç´¢ ---

def find_database_files(source_path):
    """é€’å½’æœç´¢æ–‡ä»¶å¤¹ä¸­çš„æ•°æ®åº“æ–‡ä»¶ï¼ˆ.db, .sqlite, .sqlite3ï¼‰"""
    database_files = []
    
    if os.path.isfile(source_path):
        # å¦‚æœæ˜¯å•ä¸ªæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ•°æ®åº“æ–‡ä»¶
        if source_path.lower().endswith(('.db', '.sqlite', '.sqlite3')):
            database_files.append(source_path)
        else:
            print(f"âœ– é”™è¯¯ï¼šæ–‡ä»¶ '{source_path}' ä¸æ˜¯æ”¯æŒçš„æ•°æ®åº“æ–‡ä»¶æ ¼å¼ï¼ˆ.db, .sqlite, .sqlite3ï¼‰")
    elif os.path.isdir(source_path):
        # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œé€’å½’æœç´¢æ‰€æœ‰æ•°æ®åº“æ–‡ä»¶
        print(f"ğŸ” æ­£åœ¨æœç´¢æ–‡ä»¶å¤¹ '{source_path}' ä¸­çš„æ•°æ®åº“æ–‡ä»¶...")
        
        # ä½¿ç”¨ glob é€’å½’æœç´¢æ‰€æœ‰æ”¯æŒçš„æ•°æ®åº“æ–‡ä»¶
        patterns = ['**/*.db', '**/*.sqlite', '**/*.sqlite3']
        for pattern in patterns:
            files = glob.glob(os.path.join(source_path, pattern), recursive=True)
            database_files.extend(files)
        
        # å»é‡ä¸¦æ’åº
        database_files = sorted(list(set(database_files)))
        
        if not database_files:
            print(f"âœ– åœ¨æ–‡ä»¶å¤¹ '{source_path}' ä¸­æœªæ‰¾åˆ°ä»»ä½•æ•°æ®åº“æ–‡ä»¶")
        else:
            print(f"âœ” æ‰¾åˆ° {len(database_files)} ä¸ªæ•°æ®åº“æ–‡ä»¶:")
            for i, db_file in enumerate(database_files, 1):
                print(f"  {i}. {db_file}")
    else:
        print(f"âœ– é”™è¯¯ï¼šè·¯å¾„ '{source_path}' ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®")
    
    return database_files

# --- æ•°æ®åº“è¿æ¥ä¸æ“ä½œ ---

@contextmanager
def sqlite_conn(db_path):
    conn = sqlite3.connect(db_path)
    try:
        conn.enable_load_extension(True)
        import sqlite_vec
        # import sqlite_lembed # è¿ç§»æ—¶é€šå¸¸ä¸éœ€è¦åŠ è½½ lembed
        sqlite_vec.load(conn)
        # sqlite_lembed.load(conn)
        print("âœ” sqlite-vec æ‰©å±•å·²æˆåŠŸåŠ è½½ã€‚")
    except Exception as e:
        print(f"ğŸŸ¡ è­¦å‘Š: åŠ è½½ sqlite-vec æ‰©å±•å¤±è´¥: {e}ã€‚å¦‚æœæ•°æ®åº“ä¸å«å‘é‡è¡¨ï¼Œå¯å¿½ç•¥æ­¤æ¶ˆæ¯ã€‚")

    try:
        yield conn
    finally:
        conn.close()

def get_sqlite_schema(conn):
    """è·å– SQLite æ•°æ®åº“ä¸­æ‰€æœ‰è¡¨çš„åç§°å’Œ CREATE è¯­å¥"""
    cursor = conn.cursor()
    cursor.execute("SELECT name, sql FROM sqlite_master WHERE (type='table' OR type='view') AND name NOT LIKE 'sqlite_%' AND sql IS NOT NULL;")
    schema = cursor.fetchall()
    if not schema:
        raise RuntimeError("æº SQLite æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨ã€‚")
    return schema

def get_vec_info(conn):
    """æ‰«ææ•°æ®åº“ä»¥æŸ¥æ‰¾ sqlite-vec è¡¨å¹¶æå–å‘é‡åˆ—ä¿¡æ¯"""
    vec_info = {}
    cursor = conn.cursor()
    cursor.execute("SELECT name, sql FROM sqlite_master WHERE sql LIKE '%USING vec0%';")
    vec_tables = cursor.fetchall()

    for table_name, create_sql in vec_tables:
        match = re.search(r'USING\s+vec0\s*\((.*)\)', create_sql, re.IGNORECASE | re.DOTALL)
        if not match:
            continue
        
        vec_info[table_name] = {'columns': []}
        defs_part = match.group(1).strip()
        columns_defs = re.split(r',(?![^\(]*\))', defs_part)

        for col_def in columns_defs:
            vec_col_match = re.match(r'([`"\[]?\w+[`"\]]?)\s+float\s*\[\s*(\d+)\s*\]', col_def.strip(), re.IGNORECASE)
            if vec_col_match:
                col_name = vec_col_match.group(1).strip('`"[]')
                dimension = int(vec_col_match.group(2))
                vec_info[table_name]['columns'].append({'name': col_name, 'dim': dimension})
    return vec_info


def migrate_to_postgres(args, db_name, source_db_path):
    """æ‰§è¡Œåˆ° PostgreSQL çš„è¿ç§»ï¼ˆæ”¯æŒ sqlite-vecï¼‰"""
    if not psycopg2:
        print("é”™è¯¯ï¼špsycopg2-binary æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install psycopg2-binary'")
        return
    if not tqdm:
        print("é”™è¯¯ï¼štqdm æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install tqdm'")
        return

    print(f"\nğŸš€ å¼€å§‹è¿ç§»åˆ° PostgreSQL...")
    conn_admin = None
    target_conn = None
    try:
        # 1. è¿æ¥åˆ° 'postgres' æ•°æ®åº“æ¥åˆ›å»ºæ–°æ•°æ®åº“
        print("æ­£åœ¨è¿æ¥åˆ°æœåŠ¡å™¨ä»¥åˆ›å»ºæ•°æ®åº“...")
        conn_admin = psycopg2.connect(
            host=args.host, port=args.port, user=args.user, password=args.password, dbname='postgres'
        )
        conn_admin.autocommit = True
        with conn_admin.cursor() as cur:
            cur.execute("SELECT 1 FROM pg_database WHERE datname = %s", (db_name,))
            if cur.fetchone():
                print(f"æ•°æ®åº“ '{db_name}' å·²å­˜åœ¨ã€‚")
            else:
                print(f"æ­£åœ¨åˆ›å»ºæ•°æ®åº“: {db_name}")
                cur.execute(f'CREATE DATABASE "{db_name}"')
                print(f"âœ” æ•°æ®åº“ '{db_name}' åˆ›å»ºæˆåŠŸã€‚")

        # 2. è¿æ¥åˆ°æ–°åˆ›å»ºçš„æ•°æ®åº“è¿›è¡Œè¿ç§»
        print(f"æ­£åœ¨è¿æ¥åˆ°æ–°æ•°æ®åº“ '{db_name}'...")
        with psycopg2.connect(host=args.host, port=args.port, user=args.user, password=args.password, dbname=db_name) as target_conn:
            with target_conn.cursor() as target_cur:
                # å¯ç”¨ pgvector æ‰©å±•
                print("  - æ­£åœ¨å¯ç”¨ pgvector æ‰©å±• (å¦‚æœå°šæœªå¯ç”¨)...")
                target_cur.execute('CREATE EXTENSION IF NOT EXISTS vector;')
                print("  âœ” pgvector æ‰©å±•å·²å°±ç»ªã€‚")

                with sqlite_conn(source_db_path) as source_conn:
                    vec_tables_info = get_vec_info(source_conn)
                    if vec_tables_info:
                        print(f"âœ” æ£€æµ‹åˆ° {len(vec_tables_info)} ä¸ª sqlite-vec è¡¨: {', '.join(vec_tables_info.keys())}")

                    source_cur = source_conn.cursor()
                    tables_schema = get_sqlite_schema(source_conn)

                    for table_name, create_sql in tables_schema:
                        if table_name == 'sqlite_sequence' or table_name.endswith(('_info', '_chunks', '_rowids')) or any(suffix in table_name for suffix in ('_metadatatext', '_metadatachunks', '_vector_chunks')):
                            continue

                        table_name_quoted = f'"{table_name.strip("`[]")}"'
                        
                        print(f"\n-- æ­£åœ¨å¤„ç†è¡¨: {table_name_quoted} --")

                        print("  - ç¿»è¯‘ Schema...")
                        pg_create_sql = translate_schema_for_postgres(create_sql)
                        
                        pg_create_sql = re.sub(
                            r'CREATE\s+TABLE\s+([`"\[]?\S+[`"\]]?)', 
                            f'CREATE TABLE {table_name_quoted}', 
                            pg_create_sql, 
                            count=1, 
                            flags=re.IGNORECASE
                        )

                        is_virtual = False
                        if 'USING vec0' in create_sql:
                            is_virtual = True
                            pg_create_sql = re.sub(r'USING\s+vec0', '', pg_create_sql, flags=re.IGNORECASE | re.DOTALL)
                            pg_create_sql = re.sub(r'VIRTUAL\s+TABLE', 'TABLE', pg_create_sql, flags=re.IGNORECASE)
                            pg_create_sql = re.sub(r'float\s*\[\s*(\d+)\s*\]', r'vector(\1)', pg_create_sql, flags=re.IGNORECASE)

                        print(f"  - æ­£åœ¨ç›®æ ‡æ•°æ®åº“ä¸­åˆ›å»ºè¡¨ {table_name_quoted}...")
                        target_cur.execute(f'DROP TABLE IF EXISTS {table_name_quoted} CASCADE;')
                        target_cur.execute(pg_create_sql)

                        print("  - æ­£åœ¨ä» SQLite å‡†å¤‡æ‰¹é‡æå–æ•°æ®...")
                        source_cur.execute(f'SELECT COUNT(*) FROM `{table_name.strip("`[]")}`')
                        total_rows = source_cur.fetchone()[0]
                        
                        if total_rows == 0:
                            print(f"  - è¡¨ {table_name_quoted} ä¸ºç©ºï¼Œè·³è¿‡æ•°æ®æ’å…¥ã€‚")
                            continue

                        source_cur.execute(f'SELECT * FROM `{table_name.strip("`[]")}`')
                        original_column_names = [desc[0] for desc in source_cur.description]
                        column_names = list(original_column_names)

                        try:
                            rowid_index = column_names.index('rowid')
                            print("  - æ£€æµ‹åˆ°å¹¶ç§»é™¤éšå¼çš„ 'rowid' åˆ—ã€‚")
                            del column_names[rowid_index]
                        except ValueError:
                            rowid_index = -1

                        print("  - æ­£åœ¨è·å– PostgreSQL ç›®æ ‡è¡¨ Schema...")
                        target_cur.execute("""
                            SELECT column_name, data_type 
                            FROM information_schema.columns 
                            WHERE table_name = %s
                            ORDER BY ordinal_position;
                        """, (table_name.strip('`"[]'),))
                        
                        target_column_info = {name.lower(): data_type for name, data_type in target_cur.fetchall()}
                        
                        try:
                            ordered_target_types = [target_column_info[col.lower()] for col in column_names]
                        except KeyError as e:
                            print(f"  ğŸ”´ é”™è¯¯: åˆ— '{e.args[0]}' åœ¨æºæ•°æ®å’Œç›®æ ‡ Schema ä¹‹é—´ä¸åŒ¹é…ã€‚")
                            raise ValueError(f"åˆ—ä¸åŒ¹é…ï¼Œæ— æ³•ç»§ç»­è¿ç§»è¡¨ {table_name}")

                        vec_info_for_table = vec_tables_info.get(table_name)
                        if vec_info_for_table:
                             print("  - æ­£åœ¨è§£ç  sqlite-vec å‘é‡æ•°æ®...")
                             vector_column_indices = {
                                i: item['dim'] 
                                for i, name in enumerate(column_names) 
                                for item in vec_info_for_table['columns'] if item['name'] == name
                            }

                        print(f"  - å¼€å§‹æ‰¹é‡æ’å…¥ {total_rows} æ¡æ•°æ®åˆ° PostgreSQL...")

                        with tqdm(total=total_rows, desc=f"  ğŸ“¤ Migrating {table_name}", unit="rows") as pbar:
                            while True:
                                rows_batch = source_cur.fetchmany(BATCH_SIZE)
                                if not rows_batch:
                                    break
                                
                                processed_rows = [list(row) for row in rows_batch]

                                if rowid_index != -1:
                                    original_rowid_index = original_column_names.index('rowid')
                                    for row in processed_rows:
                                        del row[original_rowid_index]

                                if vec_info_for_table:
                                    for row in processed_rows:
                                        for i, dim in vector_column_indices.items():
                                            blob = row[i]
                                            if isinstance(blob, bytes):
                                                num_floats = len(blob) // 4
                                                unpacked_data = list(struct.unpack(f'<{num_floats}f', blob))
                                                if num_floats != dim:
                                                    print(f"  ğŸŸ¡ è­¦å‘Š: åœ¨è¡¨ '{table_name}' ä¸­å‘ç°ä¸åŒ¹é…çš„å‘é‡ç»´åº¦ã€‚é¢„æœŸ {dim}ï¼Œå¾—åˆ° {num_floats}ã€‚å°†å¡«å……æˆ–æˆªæ–­ã€‚")
                                                    unpacked_data = (unpacked_data + [0.0] * dim)[:dim]
                                                row[i] = str(unpacked_data)

                                final_data = []
                                for row in processed_rows:
                                    coerced_row = []
                                    for i, value in enumerate(row):
                                        target_type = ordered_target_types[i]
                                        if 'vector' in target_type.lower():
                                            coerced_row.append(value)
                                        else:
                                            coerced_value = coerce_value(value, target_type)
                                            coerced_row.append(coerced_value)
                                    final_data.append(tuple(coerced_row))

                                insert_query = 'INSERT INTO {} ({}) VALUES ({})'.format(
                                    table_name_quoted,
                                    ', '.join([f'"{c}"' for c in column_names]),
                                    ', '.join(['%s'] * len(column_names))
                                )
                                
                                execute_batch(target_cur, insert_query, final_data, page_size=1000)
                                
                                pbar.update(len(rows_batch))
                        
                        print(f"  âœ” è¡¨ {table_name_quoted} æ•°æ®è¿ç§»å®Œæˆã€‚")

            target_conn.commit()
            print("\nğŸ‰ æ‰€æœ‰è¡¨è¿ç§»æˆåŠŸï¼")

    except (Exception, psycopg2.Error) as e:
        if target_conn:
            target_conn.rollback()
        cleanup_postgres_db(args, db_name)
        raise e
    finally:
        if conn_admin:
            conn_admin.close()
        if target_conn and not target_conn.closed:
            target_conn.close()

def migrate_to_clickhouse(args, db_name, source_db_path):
    """æ‰§è¡Œåˆ° ClickHouse çš„è¿ç§»ï¼ˆæ”¯æŒ sqlite-vecã€ç§»é™¤ rowid å¹¶å¼ºåˆ¶ç±»å‹è½¬æ¢ï¼‰"""
    if not Client:
        print("é”™è¯¯ï¼šclickhouse-driver æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install clickhouse-driver'")
        return
    if not tqdm:
        print("é”™è¯¯ï¼štqdm æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install tqdm'")
        return
        
    print(f"\nğŸš€ å¼€å§‹è¿ç§»åˆ° ClickHouse...")
    
    client = None
    try:
        print("æ­£åœ¨è¿æ¥åˆ°æœåŠ¡å™¨ä»¥åˆ›å»ºæ•°æ®åº“...")
        with Client(host=args.host, port=args.port, user=args.user, password=args.password) as admin_client:
            print(f"æ­£åœ¨åˆ›å»ºæ•°æ®åº“ (å¦‚æœä¸å­˜åœ¨): {db_name}")
            admin_client.execute(f'CREATE DATABASE IF NOT EXISTS `{db_name}`')
            print(f"âœ” æ•°æ®åº“ '{db_name}' å·²å°±ç»ªã€‚")

        print(f"æ­£åœ¨è¿æ¥åˆ°æ–°æ•°æ®åº“ '{db_name}'...")
        client = Client(host=args.host, port=args.port, user=args.user, password=args.password, database=db_name)

        with sqlite_conn(source_db_path) as source_conn:
            vec_tables_info = get_vec_info(source_conn)
            if vec_tables_info:
                print(f"âœ” æ£€æµ‹åˆ° {len(vec_tables_info)} ä¸ª sqlite-vec è¡¨: {', '.join(vec_tables_info.keys())}")

            source_cur = source_conn.cursor()
            tables_schema = get_sqlite_schema(source_conn)

            for table_name, create_sql in tables_schema:
                if table_name == 'sqlite_sequence' or table_name.endswith(('_info', '_chunks', '_rowids')) or any(suffix in table_name for suffix in ('_metadatatext', '_metadatachunks', '_vector_chunks')):
                    continue

                print(f"\n-- æ­£åœ¨å¤„ç†è¡¨: {table_name} --")

                print("  - ç¿»è¯‘ Schema...")
                ch_create_sql = translate_schema_for_clickhouse(create_sql)
                
                print(f"  - æ­£åœ¨ç›®æ ‡æ•°æ®åº“ä¸­åˆ›å»ºè¡¨ '{table_name}'...")
                client.execute(f"DROP TABLE IF EXISTS `{table_name}`")
                client.execute(ch_create_sql)
                
                print("  - æ­£åœ¨è·å– ClickHouse ç›®æ ‡è¡¨ Schema...")
                target_column_types = {
                    name: type_str for name, type_str in client.execute(
                        f"SELECT name, type FROM system.columns WHERE database = '{db_name}' AND table = '{table_name}'"
                    )
                }

                print("  - æ­£åœ¨ä» SQLite å‡†å¤‡æ‰¹é‡æå–æ•°æ®...")
                source_cur.execute(f'SELECT COUNT(*) FROM `{table_name.strip("`[]")}`')
                total_rows = source_cur.fetchone()[0]

                if total_rows == 0:
                    print(f"  - è¡¨ '{table_name}' ä¸ºç©ºï¼Œè·³è¿‡æ•°æ®æ’å…¥ã€‚")
                    continue

                source_cur.execute(f'SELECT * FROM `{table_name.strip("`[]")}`')
                original_column_names = [desc[0] for desc in source_cur.description]
                column_names = list(original_column_names)

                try:
                    rowid_index = column_names.index('rowid')
                    print("  - æ£€æµ‹åˆ°å¹¶ç§»é™¤éšå¼çš„ 'rowid' åˆ—ã€‚")
                    del column_names[rowid_index]
                except ValueError:
                    rowid_index = -1

                vec_info_for_table = vec_tables_info.get(table_name)
                if vec_info_for_table:
                    print("  - æ­£åœ¨è§£ç  sqlite-vec å‘é‡æ•°æ®...")
                    vector_column_indices = {
                        i: item['dim'] 
                        for i, name in enumerate(column_names) 
                        for item in vec_info_for_table['columns'] if item['name'] == name
                    }

                print(f"  - å¼€å§‹æ‰¹é‡æ’å…¥ {total_rows} æ¡æ•°æ®åˆ° ClickHouse...")

                with tqdm(total=total_rows, desc=f"  ğŸ“¤ Migrating {table_name}", unit="rows") as pbar:
                    while True:
                        rows_batch = source_cur.fetchmany(BATCH_SIZE)
                        if not rows_batch:
                            break
                        
                        processed_rows = [list(row) for row in rows_batch]

                        if rowid_index != -1:
                            original_rowid_index = original_column_names.index('rowid')
                            for row in processed_rows:
                                del row[original_rowid_index]
                        
                        if vec_info_for_table:
                            for row in processed_rows:
                                for i, dim in vector_column_indices.items():
                                    blob = row[i]
                                    if isinstance(blob, bytes):
                                        num_floats = len(blob) // 4
                                        if num_floats == dim:
                                            row[i] = list(struct.unpack(f'<{dim}f', blob))
                                        else:
                                            print(f"  ğŸŸ¡ è­¦å‘Š: åœ¨è¡¨ '{table_name}' ä¸­å‘ç°ä¸åŒ¹é…çš„å‘é‡ç»´åº¦ã€‚é¢„æœŸ {dim}ï¼Œå¾—åˆ° {num_floats}ã€‚å°†å¡«å……æˆ–æˆªæ–­ã€‚")
                                            unpacked_data = list(struct.unpack(f'<{num_floats}f', blob))
                                            row[i] = (unpacked_data + [0.0] * dim)[:dim]

                        final_data = []
                        for row in processed_rows:
                            coerced_row = []
                            for i, col_name in enumerate(column_names):
                                target_type = target_column_types.get(col_name, 'String')
                                value = row[i]
                                coerced_value = coerce_value(value, target_type)
                                coerced_row.append(coerced_value)
                            final_data.append(tuple(coerced_row))

                        if not final_data:
                            continue
                            
                        insert_statement = f"INSERT INTO `{table_name}` ({', '.join([f'`{c}`' for c in column_names])}) VALUES"
                        client.execute(insert_statement, final_data, types_check=True)
                        
                        pbar.update(len(rows_batch))
                
                print(f"  âœ” è¡¨ '{table_name}' æ•°æ®è¿ç§»å®Œæˆã€‚")
        print("\nğŸ‰ æ‰€æœ‰è¡¨è¿ç§»æˆåŠŸï¼")

    except Exception as e:
        cleanup_clickhouse_db(args, db_name)
        raise e
    finally:
        if client:
            client.disconnect()

def migrate_to_both_backends(database_files, pg_args, ch_args):
    """
    Orchestrates the migration of SQLite databases to BOTH PostgreSQL and ClickHouse.

    This function scans a source path for SQLite files, then for each file, it
    attempts to migrate it first to PostgreSQL and then to ClickHouse using
    their respective migration functions. Only if a database is successfully
    migrated to *both* backends will its name be added to the returned list.

    Args:
        database_files (List): The list of SQLite files.
        pg_args (argparse.Namespace): An object containing connection arguments
                                     for PostgreSQL (host, port, user, password).
        ch_args (argparse.Namespace): An object containing connection arguments
                                     for ClickHouse (host, port, user, password).

    Returns:
        list: A list of strings, where each string is the name of a database
              that was successfully migrated to both PostgreSQL and ClickHouse.
    """
    print("ğŸš€ Starting migration to both PostgreSQL and ClickHouse backends.")
    
    if not database_files:
        print("âœ– No database files found. Exiting.")
        return []

    successful_migrations = []
    pg_failures, ch_failures = [], []

    for i, db_file in enumerate(database_files, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ”„ Processing file {i}/{len(database_files)}: {os.path.basename(db_file)}")
        print(f"{'='*70}")

        db_name = os.path.splitext(os.path.basename(db_file))[0]
        db_name = re.sub(r'[^a-zA-Z0-9_]', '_', db_name).lower()
        
        pg_success = False
        ch_success = False

        # --- Attempt PostgreSQL Migration ---
        try:
            print(f"\n  -> Attempting migration to PostgreSQL (DB: {db_name})...")
            migrate_to_postgres(pg_args, db_name, db_file)
            pg_success = True
            print(f"  âœ” Successfully migrated '{db_name}' to PostgreSQL.")
        except Exception as e:
            print(f"  âŒ FAILED to migrate '{db_name}' to PostgreSQL.")
            print(f"     Error: {e}")
            # The cleanup function is already called within migrate_to_postgres on failure

        # --- Attempt ClickHouse Migration ---
        try:
            print(f"\n  -> Attempting migration to ClickHouse (DB: {db_name})...")
            migrate_to_clickhouse(ch_args, db_name, db_file)
            ch_success = True
            print(f"  âœ” Successfully migrated '{db_name}' to ClickHouse.")
        except Exception as e:
            print(f"  âŒ FAILED to migrate '{db_name}' to ClickHouse.")
            print(f"     Error: {e}")
            # The cleanup function is already called within migrate_to_clickhouse on failure

        # --- Final Check and Reporting ---
        print("\n  -- Summary for this file --")
        if not pg_success:
            pg_failures.append(db_name)
        if not ch_success:
            ch_failures.append(db_name)
        if pg_success and ch_success:
            successful_migrations.append(db_name)
            print(f"  âœ… SUCCESS: '{db_name}' was migrated to BOTH backends.")
        else:
            print(f"  âš ï¸ INCOMPLETE: Migration for '{db_name}' failed on at least one backend.")
            print(f"     PostgreSQL: {'Success' if pg_success else 'Failed'}")
            print(f"     ClickHouse: {'Success' if ch_success else 'Failed'}")

    return successful_migrations, pg_failures, ch_failures

def main():
    parser = argparse.ArgumentParser(
        description="ä¸€é”®å°† SQLite æ•°æ®åº“ï¼ˆæ”¯æŒ sqlite-vecï¼‰è¿ç§»åˆ° PostgreSQLã€ClickHouse æˆ–ä¸¤è€…ã€‚æ”¯æŒå•ä¸ªæ–‡ä»¶æˆ–æ–‡ä»¶å¤¹æ‰¹é‡è¿ç§»ã€‚",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # --- Source and Target Arguments ---
    parser.add_argument('--source', default='/mnt/b_public/data/ydw/Text2VectorSQL/Data_Synthesizer/pipeline/sqlite/results/spider', help="æº SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹è·¯å¾„ã€‚")
    parser.add_argument('--target', default='both', choices=['postgresql', 'clickhouse', 'both'], help="ç›®æ ‡æ•°æ®åº“ç±»å‹ã€‚é€‰æ‹© 'both' å°†è¿ç§»åˆ°ä¸¤ä¸ªåç«¯ã€‚")

    # --- Generic/Single Target Arguments (for backward compatibility) ---
    parser.add_argument('--host', help="ç›®æ ‡æ•°æ®åº“ä¸»æœºåœ°å€ (ç”¨äºå•ç›®æ ‡æ¨¡å¼)ã€‚")
    parser.add_argument('--port', type=int, help="ç›®æ ‡æ•°æ®åº“ç«¯å£ (ç”¨äºå•ç›®æ ‡æ¨¡å¼)ã€‚")
    parser.add_argument('--user', help="ç›®æ ‡æ•°æ®åº“ç”¨æˆ·å (ç”¨äºå•ç›®æ ‡æ¨¡å¼)ã€‚")
    parser.add_argument('--password', help="ç›®æ ‡æ•°æ®åº“å¯†ç  (ç”¨äºå•ç›®æ ‡æ¨¡å¼)ã€‚")

    # --- PostgreSQL Specific Arguments (for 'both' mode) ---
    pg_group = parser.add_argument_group('PostgreSQL Options (for --target=both)')
    pg_group.add_argument('--pg-host', default='localhost', help="[PostgreSQL] ä¸»æœºåœ°å€ã€‚")
    pg_group.add_argument('--pg-port', type=int, default=5432, help="[PostgreSQL] ç«¯å£ã€‚")
    pg_group.add_argument('--pg-user', default='postgres', help="[PostgreSQL] ç”¨æˆ·åã€‚")
    pg_group.add_argument('--pg-password', default='postgres', help="[PostgreSQL] å¯†ç ã€‚")

    # --- ClickHouse Specific Arguments (for 'both' mode) ---
    ch_group = parser.add_argument_group('ClickHouse Options (for --target=both)')
    ch_group.add_argument('--ch-host', default='localhost', help="[ClickHouse] ä¸»æœºåœ°å€ã€‚")
    ch_group.add_argument('--ch-port', type=int, default=9000, help="[ClickHouse] ç«¯å£ã€‚")
    ch_group.add_argument('--ch-user', default='default', help="[ClickHouse] ç”¨æˆ·åã€‚")
    ch_group.add_argument('--ch-password', default='', help="[ClickHouse] å¯†ç ã€‚")
    
    args = parser.parse_args()

    if not os.path.exists(args.source):
        print(f"âœ– é”™è¯¯ï¼šæºè·¯å¾„ '{args.source}' ä¸å­˜åœ¨ã€‚")
        return

    database_files = find_database_files(args.source)
    if not database_files:
        print("âœ– æœªæ‰¾åˆ°ä»»ä½•å¯è¿ç§»çš„æ•°æ®åº“æ–‡ä»¶ã€‚")
        return

    success_count = 0
    error_count = 0

    # --- Execution Logic ---
    if args.target == 'both':
        # Create separate config namespaces for each database
        pg_args = argparse.Namespace(host=args.pg_host, port=args.pg_port, user=args.pg_user, password=args.pg_password)
        ch_args = argparse.Namespace(host=args.ch_host, port=args.ch_port, user=args.ch_user, password=args.ch_password)
        
        successful_dbs, pg_failures, ch_failures = migrate_to_both_backends(database_files, pg_args, ch_args)
        
        success_count = len(successful_dbs)
        error_count = len(database_files) - success_count
        
        print(f"\n{'='*70}")
        print("ğŸ¯ æœ€ç»ˆè¿ç§»æŠ¥å‘Š (æ¨¡å¼: both)")
        print(f"{'='*70}")
        print(f"  æ€»å…±å¤„ç†çš„æ–‡ä»¶æ•°: {len(database_files)}")
        print(f"  å®Œå…¨æˆåŠŸ (è¿ç§»åˆ°ä¸¤ä¸ªåç«¯): {success_count}")
        print(f"  éƒ¨åˆ†æˆ–å®Œå…¨å¤±è´¥: {error_count}")
        if successful_dbs:
            print("\n  æˆåŠŸè¿ç§»çš„æ•°æ®åº“ååˆ—è¡¨:")
            for db in successful_dbs:
                print(f"    - {db}")
        if pg_failures:
            print("\n  PostgreSQL è¿ç§»å¤±è´¥çš„æ•°æ®åº“ååˆ—è¡¨:")
            for db in pg_failures:
                print(f"    - {db}")
        if ch_failures:
            print("\n  ClickHouse è¿ç§»å¤±è´¥çš„æ•°æ®åº“ååˆ—è¡¨:")
            for db in ch_failures:
                print(f"    - {db}")
        print(f"{'='*70}")

    else: # Logic for single target migration
        # Populate single-target args if not provided
        if args.host is None:
            args.host = 'localhost'
        if args.port is None:
            args.port = 5432 if args.target == 'postgresql' else 9000
        if args.user is None:
            args.user = 'postgres' if args.target == 'postgresql' else 'default'
        if args.password is None:
            args.password = 'postgres' if args.target == 'postgresql' else ''
        
        print(f"\nğŸ“Š è¿ç§»ä»»åŠ¡:")
        print(f"  æºè·¯å¾„: {args.source}")
        print(f"  ç›®æ ‡ç±»å‹: {args.target}")
        print(f"  ç›®æ ‡ä¸»æœº: {args.host}:{args.port}")
        print(f"  æ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶: {len(database_files)} ä¸ª")

        for i, db_file in enumerate(database_files, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ”„ æ­£åœ¨å¤„ç†æ•°æ®åº“ {i}/{len(database_files)}: {os.path.basename(db_file)}")
            print(f"{'='*60}")
            
            try:
                db_name = os.path.splitext(os.path.basename(db_file))[0]
                db_name = re.sub(r'[^a-zA-Z0-9_]', '_', str(db_name)).lower()
                print(f"ç›®æ ‡æ•°æ®åº“å: {db_name}")
                
                if args.target == 'postgresql':
                    migrate_to_postgres(args, db_name, db_file)
                elif args.target == 'clickhouse':
                    migrate_to_clickhouse(args, db_name, db_file)
                
                success_count += 1
                print(f"âœ… æ•°æ®åº“ {i}/{len(database_files)} è¿ç§»æˆåŠŸ: {os.path.basename(db_file)}")
                
            except Exception as e:
                error_count += 1
                print(f"âŒ æ•°æ®åº“ {i}/{len(database_files)} è¿ç§»å¤±è´¥: {os.path.basename(db_file)}")
                print(f"   é”™è¯¯è¯¦æƒ…: {e}")
                import traceback
                traceback.print_exc()
        
        # Display final statistics for single mode
        print(f"\n{'='*60}")
        print(f"ğŸ¯ è¿ç§»å®Œæˆç»Ÿè®¡ (æ¨¡å¼: {args.target}):")
        print(f"   æ€»æ–‡ä»¶æ•°: {len(database_files)}")
        print(f"   æˆåŠŸ: {success_count}")
        print(f"   å¤±è´¥: {error_count}")
        print(f"{'='*60}")

    if error_count > 0:
        print(f"âš ï¸  æœ‰ {error_count} ä¸ªæ•°æ®åº“è¿ç§»ä»»åŠ¡æœªèƒ½å®Œå…¨æˆåŠŸï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
    else:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®åº“è¿ç§»ä»»åŠ¡å‡å·²æˆåŠŸï¼")

# The rest of the script (all other functions) remains the same.
# You just need to replace the original main() with this new version
# and add the migrate_to_both_backends() function anywhere in the file.

if __name__ == '__main__':
    main()