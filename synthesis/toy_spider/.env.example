# --- VLLM Service Configuration ---(可选)
# 这是您本地运行的VLLM OpenAI兼容服务器的URL
VLLM_API_URL="http://127.0.0.1:8000/v1"
# 这是VLLM加载的模型路径，它会显示在API中，通常是原始模型名
# 在您的例子中，可能是 "Qwen/Qwen1.5-235B-Thinking" 或您量化后模型的路径
VLLM_MODEL_NAME="/mnt/b_public/data/ydw/model/Qwen/Qwen2.5-72B-Instruct"


# --- Openai Service Configuration ---(可选)
API_KEY = "sk-xxx"  # 替换为您的API Key
BASE_URL = "http://123.129.219.111:3000/v1"      # 替换为您的API Base URL
LLM_MODEL_NAME = "gpt-4o"  # 用于生成新问题和VectorSQL的模型
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"  # 在VectorSQL中使用的嵌入模型名
# 是否禁用并行处理 (true 或 false)
# 默认为 false，即开启并行处理
NO_PARALLEL="false"

# --- Script Settings ---
# 并行请求数量，可以根据您大模型服务的承载能力调整
MAX_WORKERS=32

# 缓存文件路径，防止中断后从头开始
CACHE_FILE_PATH="cache/bird/disk_cache"

# enhance_tables_json.py
ENHANCE_TABLE_MODE="enhance_vec"
# "enhance_bird": 为BIRD原始数据集添加每个表格的样例数据,
# "enhance_spider": 为spider原始数据集添加每个表格的样例数据,
# "spider_vector": 为向量化后的spider数据集添加每个表格的样例数据


# find_semantic_rich_column.py
# --- 选填参数 ---
# 是否禁用并行处理 (true 或 false)
# 默认为 false，即开启并行处理
NO_PARALLEL_FIND_SEMANTIC_RICH="false"
# 输入文件的路径
# 默认为 "./results/after_add_description_tables.json"
INPUT_FILE_FIND_SEMANTIC_RICH="./results/enhanced_train_tables.json"
# 输出文件的路径
# 默认为 "./results/embedding_after_add_description_tables.json"
OUTPUT_FILE_FIND_SEMANTIC_RICH="./results/find_semantic_tables.json"



# batch_vectorize_databases.py
# --- Configuration ---
SOURCE_DB_ROOT = "train/toy_spider"
SQL_SCRIPT_DIR = "results/vector_sql_toy"
VECTOR_DB_ROOT = "results/vector_databases_toy"
TABLE_JSON_PATH = "./results/find_semantic_tables.json"
model_path = '/mnt/b_public/data/yaodongwen/model'

# generate_vector_schema.py
# --- Configuration ---
# Directory where your final vector databases are stored
VECTOR_DB_ROOT_GENERATE_SCHEMA = "results/vector_databases_toy"
# Path to the original schema file, used for reference
ORIGINAL_SCHEMA_PATH = "results/find_semantic_tables.json"
# The name of the output file you want to create
OUTPUT_DIR_GENERATE_SCHEMA = "./results"
OUTPUT_JSON_PATH_GENERATE_SCHEMA = "embedding_table_vector.json"

#enhance_tables_json.py
# 包含数据库的根目录
BASE_DIR_ENHANCE_VECTOR="results"
# 原始表的 schema json 文件路径
TABLE_SCHEMA_PATH_ENHANCE_VECTOR="find_semantic_tables.json"
# 数据库位置
DATAPATH_PATH_ENHANCE_VECTOR="vector_databases_arxiv"
# 输出目录
OUTPUT_DIR_ENHANCE_VECTOR="./results"
# 输出 schema 文件的名称
OUTPUT_SCHEMA_NAME_ENHANCE_VECTOR="enhanced_embedding_table_vector.json"



# sql_vectorize.py
SCHEMA_FILE_PATH = "./results/spider_json/enhanced_new_embedding_after_add_description_tables_spider.json"
INPUT_FILE_PATH = "results/spider_json/vector_train_spider.json"
OUTPUT_FILE_PATH = "results/spider_json/vector_train_semantic_spider.json"

# --- Configuration for post_process_sqls.py ---
# Directory containing the SQLite database subdirectories
DB_DIR_PS="./results/vector_databases_toy"
# Path to the input JSON file generated by the LLM (ndjson format)
LLM_JSON_PATH_PS="./results/sql_synthesis.json"
# Path for the final output JSON file
OUTPUT_JSON_PATH_PS="./results/synthetic_sqls.json"
# Number of CPU cores to use for parallel processing
NUM_CPUS_PS=10
# Timeout in seconds for a single SQL query execution
SQL_EXEC_TIMEOUT_PS=60


# --- Configuration for synthesize_candidate.py ---
# Number of SQL candidates to generate for each question
NUM_CANDIDATES=5
