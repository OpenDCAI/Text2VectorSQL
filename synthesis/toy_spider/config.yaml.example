sqlite:
  base_dir: "./sqlite"
  toy_spider:
    # --------------------------------------------------------------------
    # 服务配置 (Services Configuration)
    # 在vllm和openai中二选一
    # --------------------------------------------------------------------
    services:
      # VLLM 服务配置 (可选)
      vllm:
        api_url: "http://127.0.0.1:8000/v1"
        model_name: "/mnt/b_public/data/ydw/model/Qwen/Qwen2.5-72B-Instruct"
      
      # OpenAI 兼容服务配置 (可选)
      openai:
        api_key: "sk-xxx"
        base_url: "http://123.129.219.111:3000/v1"
        llm_model_name: "gpt-4o"
        embedding_model_name: "all-MiniLM-L6-v2"

    # --------------------------------------------------------------------
    # 路径配置 (Paths Configuration)
    # --------------------------------------------------------------------
    paths:
      # 产生的tables.json的位置 （generate_schema.py）
      generate_tables_json_path: "sqlite/train/toy_spider/tables.json"

      # 存放结果的目录位置
      result_path: "sqlite/results/toy_spider"

      # enhance_tables_toy.py
      enhance_json_name: "enhanced_train_tables.json"

      # 原始数据库的根目录 (batch_vectorize_databases.py)
      source_db_root: "sqlite/train/toy_spider"
      # 生成的矢量化SQL脚本目录 (batch_vectorize_databases.py)
      sql_script_dir: "sqlite/results/toy_spider/vector_sql_toy"
      # 最终生成的向量数据库目录 (batch_vectorize_databases.py & generate_vector_schema.py)
      vector_db_root: "sqlite/results/toy_spider/vector_databases_toy"
      # 包含语义列信息的表JSON文件路径 (batch_vectorize_databases.py)

      # find_semantic_rich_column.py
      find_semantic_table_json: "sqlite/results/toy_spider/find_semantic_tables.json"
      find_semantic_prompt_template_path: "sqlite/prompt_templates/find_semantic_rich_column.txt"
      # 模型下载/缓存路径 (batch_vectorize_databases.py)
      model_download: '/mnt/b_public/data/yaodongwen/model'
      # 缓存文件路径 (通用)
      cache_file: "sqlite/cache/toy_spider/disk_cache"

      # 原始schema文件路径，用于参考 (generate_vector_schema.py)等于find_semantic_table_json
      original_schema: "sqlite/results/toy_spider/find_semantic_tables.json"
      # schema文件的输出目录 (generate_vector_schema.py)
      schema_output_dir: "sqlite/results/toy_spider"
      # schema文件的输出文件名 (generate_vector_schema.py)
      schema_output_json: "embedding_table_vector.json"
      
      # generate_sql_synthesis_prompts.py
      # 第一个参数为vector_db_root
      prompt_tpl_path: "sqlite/prompt_templates/sql_synthesis_prompt.txt"
      functions_path: "sqlite/prompt_templates/sqlite_funcs.json"
      sql_prompts_output_dir: "sqlite/prompts/toy_spider"
      sql_prompts_output_name: "sql_synthesis_prompts.json"

      # synthesize_sql.py
      synthesize_sql_input_file: "sqlite/prompts/toy_spider/sql_synthesis_prompts.json"
      synthesize_sql_output_file: "sqlite/results/toy_spider/sql_synthesis.json"

      # post_process_sqls.py
      EMBED_MODEL_PATH: "../../model/all-MiniLM-L6-v2.e4ce9877.q8_0.gguf"
      post_sql_output_path: "sqlite/results/toy_spider/synthetic_sqls.json"
      post_sql_llm_json_path: "sqlite/results/toy_spider/sql_synthesis.json"

      # (enhance_tables_json.py 'enhance_vec' mode)
      # enhance_vector:
      #   base_dir: "sqlite/results/toy_spider"
      #   table_schema_path: "find_semantic_tables.json"
      #   datapath: "vector_databases_toy"
      #   output_dir: "sqlite/results/toy_spider"
      #   output_schema_name: "enhanced_embedding_table_vector.json"

      # generate_question_synthesis_prompts.py
      sql_infos_path: "sqlite/results/toy_spider/synthetic_sqls.json"
      question_synthesis_template_path: "sqlite/prompt_templates/question_synthesis_prompt.txt"
      question_prompts_output_json_path: "sqlite/prompts/toy_spider/question_synthesis_prompts.json"

      # synthesize_question.py
      synthesize_question_input_file: "sqlite/prompts/toy_spider/question_synthesis_prompts.json"
      synthesize_question_output_file: "sqlite/results/toy_spider/question_synthesis.json"

      # post_process_questions.py
      post_process_questions_input_dataset_path: "sqlite/results/toy_spider/question_synthesis.json"
      post_process_questions_output_file: "sqlite/results/toy_spider/question_and_sql_pairs.json"
      model_name_or_path: "sentence-transformers/all-mpnet-base-v2"

      # synthesiaze_candidate.py
      synthesiaze_candidate_input_file: "sqlite/results/toy_spider/question_and_sql_pairs.json"
      synthesiaze_candidate_output_file: "sqlite/results/toy_spider/candidate_sql.json"

      # generate_cot_synthesisis_prompts.py
      gene_cot_prompts_dataset_json_path: "sqlite/results/toy_spider/question_and_sql_pairs.json"
      gene_cot_prompts_tables_json_path: "sqlite/results/toy_spider/embedding_table_vector.json"
      gene_cot_prompts_prompt_tamplate_path: "sqlite/prompt_templates/cot_synthesis_prompt_template.txt"
      gene_cot_prompts_output_prompt_path: "sqlite/prompts/toy_spider/cot_synthesis_prompts.json"

      # synthesize_cot.py
      synthesize_cot_output_file: "sqlite/prompts/toy_spider/cot_synthesis.json"
      cache_file_path_cot: "sqlite/cache/toy_spider/disk_cache_cot"

      # post_process_cot.py
      post_process_cot_results_path: "sqlite/results/toy_spider/cot_synthesis.json"
      post_process_cot_db_dir: "sqlite/results/toy_spider/vector_databases_toy"
      post_process_cot_output_dir: "sqlite/results/toy_spider"

      # (sql_vectorize.py)
      # sql_vectorize:
      #   schema_file: "sqlite/results/toy_spider/spider_json/enhanced_new_embedding_after_add_description_tables_spider.json"
      #   input_file: "sqlite/results/toy_spider/spider_json/vector_train_spider.json"
      #   output_file: "sqlite/results/toy_spider/spider_json/vector_train_semantic_spider.json"

      

    # --------------------------------------------------------------------
    # 参数配置 (Parameters Configuration)
    # --------------------------------------------------------------------
    parameters:
      # 并行请求数量
      max_workers: 32
      # 为哪个数据集模式增强表信息 (enhance_tables_json.py)
      enhance_table_mode: "enhance_vec"
      # 是否禁用并行处理 (find_semantic_rich_column.py)
      no_parallel_find_semantic_rich: false
      # CPU核心数 (post_process_sqls_new.py)
      num_cpus: 10
      # SQL执行超时秒数 (post_process_sqls_new.py)
      sql_exec_timeout: 60
      # 每个问题生成的SQL候选数量 (synthesize_candidate.py)
      num_candidates: 5
