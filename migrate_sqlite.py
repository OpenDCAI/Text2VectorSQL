import argparse
import os
import sqlite3
import re
import struct
from contextlib import contextmanager

# å°è¯•å¯¼å…¥ä¾èµ–ï¼Œå¦‚æœå¤±è´¥åˆ™ç»™å‡ºæç¤º
try:
    import psycopg2
    from psycopg2.extras import execute_batch
except ImportError:
    psycopg2 = None
    execute_batch = None

try:
    from clickhouse_driver import Client
except ImportError:
    Client = None

# --- Schema ç¿»è¯‘æ¨¡å— ---

def translate_type_for_postgres(sqlite_type):
    """å°† SQLite æ•°æ®ç±»å‹æ˜ å°„åˆ° PostgreSQL ç±»å‹"""
    sqlite_type = sqlite_type.upper()
    if 'INT' in sqlite_type:
        return 'BIGINT'
    if 'CHAR' in sqlite_type or 'TEXT' in sqlite_type or 'CLOB' in sqlite_type:
        return 'TEXT'
    if 'BLOB' in sqlite_type:
        return 'BYTEA'
    if 'REAL' in sqlite_type or 'FLOA' in sqlite_type or 'DOUB' in sqlite_type:
        return 'DOUBLE PRECISION'
    if 'NUMERIC' in sqlite_type or 'DECIMAL' in sqlite_type:
        return 'NUMERIC'
    return 'TEXT' # é»˜è®¤å›é€€

def translate_schema_for_postgres(create_sql):
    """å°† SQLite çš„ CREATE TABLE è¯­å¥ç¿»è¯‘ä¸º PostgreSQL å…¼å®¹æ ¼å¼"""
    # ç»Ÿä¸€å°† SQLite çš„å¼•ç”¨ç¬¦å· ` æ›¿æ¢ä¸º PostgreSQL çš„ "
    create_sql = create_sql.replace('`', '"')
    
    # ç§»é™¤ AUTOINCREMENT å…³é”®å­—ï¼ŒPostgreSQL ä½¿ç”¨ SERIAL/BIGSERIAL
    create_sql = re.sub(r'\bAUTOINCREMENT\b', '', create_sql, flags=re.IGNORECASE)
    # å°† INTEGER PRIMARY KEY è½¬æ¢ä¸º SERIAL PRIMARY KEY
    create_sql = re.sub(r'INTEGER\s+PRIMARY\s+KEY', 'SERIAL PRIMARY KEY', create_sql, flags=re.IGNORECASE)

    # ä» CREATE è¯­å¥ä¸­æå–åˆ—å®šä¹‰éƒ¨åˆ†
    try:
        table_name_match = re.search(r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?\s*([`"\[]\S+[`"\]]|\S+)\s*(\(.*\))', create_sql, flags=re.IGNORECASE | re.DOTALL)
        if not table_name_match:
            return create_sql
            
        table_name = table_name_match.group(1)
        defs_part = table_name_match.group(2)
        
        defs_content = defs_part.strip()[1:-1]
        
        defs_list = re.split(r',(?![^\(]*\))', defs_content)

        new_defs = []
        for definition in defs_list:
            definition = definition.strip()
            if definition.upper().startswith(('PRIMARY', 'FOREIGN', 'UNIQUE', 'CONSTRAINT', 'CHECK')):
                new_defs.append(definition)
                continue

            col_match = re.match(r'([`"\[]?\w+[`"\]]?)\s+([A-Z]+(?:\(\d+(?:,\d+)?\))?)(.*)', definition, flags=re.IGNORECASE)
            
            if col_match:
                col_name, col_type, constraints = col_match.groups()
                if 'SERIAL PRIMARY KEY' in definition.upper():
                    new_defs.append(definition)
                else:
                    pg_type = translate_type_for_postgres(col_type)
                    new_defs.append(f'{col_name} {pg_type}{constraints}')
            else:
                new_defs.append(definition)

        return f"CREATE TABLE {table_name} ({', '.join(new_defs)})"
    except Exception:
        return re.sub(r'\bINTEGER\b', 'BIGINT', create_sql, flags=re.IGNORECASE)


def translate_type_for_clickhouse(sqlite_type):
    """å°† SQLite æ•°æ®ç±»å‹æ˜ å°„åˆ° ClickHouse ç±»å‹"""
    sqlite_type = sqlite_type.upper()
    if 'INT' in sqlite_type: return 'Int64'
    if 'CHAR' in sqlite_type or 'TEXT' in sqlite_type or 'CLOB' in sqlite_type: return 'String'
    if 'BLOB' in sqlite_type: return 'String' # é»˜è®¤ä¸º Stringï¼Œå‘é‡ BLOB ä¼šè¢«ç‰¹æ®Šå¤„ç†
    if 'REAL' in sqlite_type or 'FLOA' in sqlite_type or 'DOUB' in sqlite_type: return 'Float64'
    if 'NUMERIC' in sqlite_type or 'DECIMAL' in sqlite_type: return 'Decimal(38, 6)'
    if 'DATE' in sqlite_type: return 'Date'
    if 'DATETIME' in sqlite_type: return 'DateTime'
    return 'String'

def translate_schema_for_clickhouse(create_sql):
    """å°† SQLite çš„ CREATE TABLE è¯­å¥ï¼ˆåŒ…æ‹¬ vec0 è™šæ‹Ÿè¡¨ï¼‰ç¿»è¯‘ä¸º ClickHouse å…¼å®¹æ ¼å¼"""
    # æ›´æ–°æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ¹é… "CREATE TABLE" æˆ– "CREATE VIRTUAL TABLE"
    # table_name_match = re.search(
    #     r'CREATE\s+(?:VIRTUAL\s+)?TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?\s*([`"\[]\S+[`"\]]|\S+)\s*\(',
    #     create_sql, flags=re.IGNORECASE
    # )
    table_name_match = re.search(
        r'CREATE\s+(?:VIRTUAL\s+)?TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?\s*([`"\[]\S+[`"\]]|\S+)',
        create_sql, flags=re.IGNORECASE
    )
    if not table_name_match:
        raise ValueError(f"æ— æ³•ä» SQL ä¸­è§£æè¡¨å: {create_sql}")
        
    table_name = table_name_match.group(1).strip('`"[]')

    lines = []
    indices = []

    columns_part_match = re.search(r'\((.*)\)', create_sql, re.DOTALL)
    if not columns_part_match:
        raise ValueError(f"æ— æ³•ä» SQL ä¸­è§£æåˆ—å®šä¹‰: {create_sql}")
    columns_part = columns_part_match.group(1)
    
    columns_defs = re.split(r',(?![^\(]*\))', columns_part)

    for col_def in columns_defs:
        col_def = col_def.strip()
        if not col_def or col_def.upper().startswith(('PRIMARY', 'FOREIGN', 'UNIQUE', 'CONSTRAINT', 'CHECK')):
            continue

        # ä¼˜å…ˆåŒ¹é… sqlite-vec çš„å‘é‡åˆ—è¯­æ³•, e.g., "col_embedding float[384]"
        vec_col_match = re.match(r'([`"\[]?\w+[`"\]]?)\s+float\s*\[\s*(\d+)\s*\]', col_def, re.IGNORECASE)
        if vec_col_match:
            col_name = vec_col_match.group(1).strip('`"')
            dimension = int(vec_col_match.group(2))
            
            # ç¿»è¯‘ä¸º ClickHouse å‘é‡ç±»å‹
            lines.append(f'`{col_name}` Array(Float32)')
            
            # ä¸ºè¯¥åˆ—è‡ªåŠ¨æ·»åŠ å‘é‡ç´¢å¼•
            indices.append(
                f"INDEX `idx_vec_{col_name}` `{col_name}` TYPE vector_similarity('hnsw', 'L2Distance', {dimension}) GRANULARITY 1000"
            )
        else:
            # å¤„ç†æ™®é€šåˆ—
            parts = re.split(r'\s+', col_def, 2)
            col_name = parts[0].strip('`"')
            
            if len(parts) > 1:
                sqlite_type = parts[1].split('(')[0]
                ch_type = translate_type_for_clickhouse(sqlite_type)
                if 'NOT NULL' not in col_def.upper():
                    ch_type = f'Nullable({ch_type})'
                lines.append(f'`{col_name}` {ch_type}')

    create_table_ch = f"CREATE TABLE `{table_name}` (\n    "
    all_definitions = lines + indices
    create_table_ch += ',\n    '.join(all_definitions)
    create_table_ch += f"\n) ENGINE = MergeTree() ORDER BY tuple();"
    
    return create_table_ch


# --- æ•°æ®åº“è¿æ¥ä¸æ“ä½œ ---

@contextmanager
def sqlite_conn(db_path):
    conn = sqlite3.connect(db_path)
    conn.enable_load_extension(True)
    import sqlite_vec
    import sqlite_lembed
    sqlite_vec.load(conn)
    sqlite_lembed.load(conn)
    print("sqlite-vec æ‰©å±•å·²æˆåŠŸåŠ è½½ã€‚")
    try:
        yield conn
    finally:
        conn.close()

def get_sqlite_schema(conn):
    """è·å– SQLite æ•°æ®åº“ä¸­æ‰€æœ‰è¡¨çš„åç§°å’Œ CREATE è¯­å¥"""
    cursor = conn.cursor()
    # åŒæ—¶è·å–æ™®é€šè¡¨å’Œè™šæ‹Ÿè¡¨
    cursor.execute("SELECT name, sql FROM sqlite_master WHERE (type='table' OR type='view') AND name NOT LIKE 'sqlite_%';")
    schema = cursor.fetchall()
    if not schema:
        raise RuntimeError("æº SQLite æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨ã€‚")
    return schema

def get_vec_info(conn):
    """æ‰«ææ•°æ®åº“ä»¥æŸ¥æ‰¾ sqlite-vec è¡¨å¹¶æå–å‘é‡åˆ—ä¿¡æ¯"""
    vec_info = {}
    cursor = conn.cursor()
    cursor.execute("SELECT name, sql FROM sqlite_master WHERE sql LIKE '%USING vec0%';")
    vec_tables = cursor.fetchall()

    for table_name, create_sql in vec_tables:
        match = re.search(r'USING\s+vec0\s*\((.*)\)', create_sql, re.IGNORECASE | re.DOTALL)
        if not match:
            continue
        
        vec_info[table_name] = {'columns': []}
        defs_part = match.group(1).strip()
        columns_defs = re.split(r',(?![^\(]*\))', defs_part)

        for col_def in columns_defs:
            vec_col_match = re.match(r'([`"\[]?\w+[`"\]]?)\s+float\s*\[\s*(\d+)\s*\]', col_def.strip(), re.IGNORECASE)
            if vec_col_match:
                col_name = vec_col_match.group(1).strip('`"[]')
                dimension = int(vec_col_match.group(2))
                vec_info[table_name]['columns'].append({'name': col_name, 'dim': dimension})
    return vec_info


def migrate_to_postgres(args, db_name):
    """æ‰§è¡Œåˆ° PostgreSQL çš„è¿ç§»"""
    if not psycopg2:
        print("é”™è¯¯ï¼špsycopg2-binary æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install psycopg2-binary'")
        return

    print(f"\nğŸš€ å¼€å§‹è¿ç§»åˆ° PostgreSQL...")
    conn_admin = None
    target_conn = None
    try:
        # 1. è¿æ¥åˆ° 'postgres' æ•°æ®åº“æ¥åˆ›å»ºæ–°æ•°æ®åº“
        print("æ­£åœ¨è¿æ¥åˆ°æœåŠ¡å™¨ä»¥åˆ›å»ºæ•°æ®åº“...")
        conn_admin = psycopg2.connect(
            host=args.host, port=args.port, user=args.user, password=args.password, dbname='postgres'
        )
        conn_admin.autocommit = True
        with conn_admin.cursor() as cur:
            cur.execute(f"SELECT 1 FROM pg_database WHERE datname = %s", (db_name,))
            if cur.fetchone():
                print(f"æ•°æ®åº“ '{db_name}' å·²å­˜åœ¨ã€‚")
            else:
                print(f"æ­£åœ¨åˆ›å»ºæ•°æ®åº“: {db_name}")
                cur.execute(f"CREATE DATABASE \"{db_name}\"") # ä½¿ç”¨å¼•å·ä»¥é˜²ç‰¹æ®Šå­—ç¬¦
                print(f"âœ” æ•°æ®åº“ '{db_name}' åˆ›å»ºæˆåŠŸã€‚")

        # 2. è¿æ¥åˆ°æ–°åˆ›å»ºçš„æ•°æ®åº“è¿›è¡Œè¿ç§»
        print(f"æ­£åœ¨è¿æ¥åˆ°æ–°æ•°æ®åº“ '{db_name}'...")
        with psycopg2.connect(host=args.host, port=args.port, user=args.user, password=args.password, dbname=db_name) as target_conn:
            with target_conn.cursor() as target_cur:
                with sqlite_conn(args.source) as source_conn:
                    source_cur = source_conn.cursor()
                    tables_schema = get_sqlite_schema(source_conn)

                    print(f"å…±æ‰¾åˆ° {len(tables_schema)} ä¸ªè¡¨éœ€è¦è¿ç§»ã€‚")

                    for table_name, create_sql in tables_schema:
                        table_name_quoted = f'"{table_name.strip("`[]")}"'
                        
                        print(f"\n-- æ­£åœ¨å¤„ç†è¡¨: {table_name_quoted} --")

                        print("  - ç¿»è¯‘ Schema...")
                        pg_create_sql = translate_schema_for_postgres(create_sql)
                        pg_create_sql = re.sub(
                            r'CREATE\s+TABLE\s+([`"\[]?\S+[`"\]]?)', 
                            f'CREATE TABLE {table_name_quoted}', 
                            pg_create_sql, 
                            count=1, 
                            flags=re.IGNORECASE
                        )

                        print(f"  - æ­£åœ¨ç›®æ ‡æ•°æ®åº“ä¸­åˆ›å»ºè¡¨ {table_name_quoted}...")
                        target_cur.execute(f'DROP TABLE IF EXISTS {table_name_quoted} CASCADE;')
                        target_cur.execute(pg_create_sql)

                        print("  - ä» SQLite ä¸­æå–æ•°æ®...")
                        source_cur.execute(f'SELECT * FROM `{table_name.strip("`[]")}`')
                        data = source_cur.fetchall()

                        if not data:
                            print(f"  - è¡¨ {table_name_quoted} ä¸ºç©ºï¼Œè·³è¿‡æ•°æ®æ’å…¥ã€‚")
                            continue

                        cols_count = len(source_cur.description)
                        insert_query = f'INSERT INTO {table_name_quoted} VALUES ({", ".join(["%s"] * cols_count)})'
                        
                        print(f"  - æ‰¹é‡æ’å…¥ {len(data)} æ¡æ•°æ®åˆ° PostgreSQL...")
                        execute_batch(target_cur, insert_query, data, page_size=1000)
                        print(f"  âœ” è¡¨ {table_name_quoted} æ•°æ®è¿ç§»å®Œæˆã€‚")
            
            target_conn.commit()
            print("\nğŸ‰ æ‰€æœ‰è¡¨è¿ç§»æˆåŠŸï¼")

    except (Exception, psycopg2.Error) as e:
        if target_conn:
            target_conn.rollback()
        print(f"\nâœ– è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if conn_admin:
            conn_admin.close()
        if target_conn and not target_conn.closed:
            target_conn.close()

def migrate_to_clickhouse(args, db_name):
    """æ‰§è¡Œåˆ° ClickHouse çš„è¿ç§»ï¼ˆæ”¯æŒ sqlite-vecã€ç§»é™¤ rowid å¹¶å¼ºåˆ¶ç±»å‹è½¬æ¢ï¼‰"""
    if not Client:
        print("é”™è¯¯ï¼šclickhouse-driver æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install clickhouse-driver'")
        return
        
    print(f"\nğŸš€ å¼€å§‹è¿ç§»åˆ° ClickHouse...")
    
    client = None
    try:
        # 1. è¿æ¥åˆ°é»˜è®¤æ•°æ®åº“ï¼Œä»…ç”¨äºåˆ›å»ºæ–°åº“
        print("æ­£åœ¨è¿æ¥åˆ°æœåŠ¡å™¨ä»¥åˆ›å»ºæ•°æ®åº“...")
        with Client(host=args.host, port=args.port, user=args.user, password=args.password) as admin_client:
            print(f"æ­£åœ¨åˆ›å»ºæ•°æ®åº“ (å¦‚æœä¸å­˜åœ¨): {db_name}")
            admin_client.execute(f'CREATE DATABASE IF NOT EXISTS `{db_name}`')
            print(f"âœ” æ•°æ®åº“ '{db_name}' å·²å°±ç»ªã€‚")

        # 2. è¿æ¥åˆ°æ–°åˆ›å»ºçš„æ•°æ®åº“ï¼Œæ‰§è¡Œæ‰€æœ‰è¡¨æ“ä½œ
        print(f"æ­£åœ¨è¿æ¥åˆ°æ–°æ•°æ®åº“ '{db_name}'...")
        client = Client(host=args.host, port=args.port, user=args.user, password=args.password, database=db_name)

        with sqlite_conn(args.source) as source_conn:
            # å°è¯•åŠ è½½ sqlite-vec æ‰©å±•
            source_conn.enable_load_extension(True)
            try:
                import sqlite_vec
                import sqlite_lembed
                sqlite_vec.load(source_conn)
                sqlite_lembed.load(source_conn)
                print("âœ” sqlite-vec æ‰©å±•å·²æˆåŠŸåŠ è½½ã€‚")
            except sqlite3.OperationalError:
                print("ğŸŸ¡ è­¦å‘Š: 'vec0' æ‰©å±•æœªæ‰¾åˆ°ã€‚å¦‚æœæ•°æ®åº“ä¸å«å‘é‡è¡¨ï¼Œå¯å¿½ç•¥æ­¤æ¶ˆæ¯ã€‚")

            vec_tables_info = get_vec_info(source_conn)
            if vec_tables_info:
                print(f"âœ” æ£€æµ‹åˆ° {len(vec_tables_info)} ä¸ª sqlite-vec è¡¨: {', '.join(vec_tables_info.keys())}")

            source_cur = source_conn.cursor()
            tables_schema = get_sqlite_schema(source_conn)
            # print(f"å…±æ‰¾åˆ° {len(tables_schema)} ä¸ªè¡¨éœ€è¦è¿ç§»ã€‚")

            for table_name, create_sql in tables_schema:
                # å¦‚æœtable_nameåŒ…å« '_metadatatext', '_metadatachunks', '_vector_chunks' åˆ™è·³è¿‡
                # å¦‚æœtable_nameä»¥ '_info', '_chunks', "_rowids" ç»“å°¾åˆ™è·³è¿‡
                # å¦‚æœtable_nameç­‰äº'sqlite_sequence'åˆ™è·³è¿‡
                if table_name == 'sqlite_sequence' or table_name.endswith(('_info', '_chunks', '_rowids')) or any(suffix in table_name for suffix in ('_metadatatext', '_metadatachunks', '_vector_chunks')):
                    # print(f"  - è·³è¿‡ç³»ç»Ÿè¡¨: {table_name}")
                    continue

                print(f"\n-- æ­£åœ¨å¤„ç†è¡¨: {table_name} --")

                print("  - ç¿»è¯‘ Schema...")
                ch_create_sql = translate_schema_for_clickhouse(create_sql)
                
                print(f"  - æ­£åœ¨ç›®æ ‡æ•°æ®åº“ä¸­åˆ›å»ºè¡¨ '{table_name}'...")
                client.execute(f"DROP TABLE IF EXISTS `{table_name}`")
                client.execute(ch_create_sql)
                
                # --- æ–°å¢é€»è¾‘ï¼šè·å– ClickHouse ç›®æ ‡è¡¨çš„å‡†ç¡® Schema ---
                print("  - æ­£åœ¨è·å– ClickHouse ç›®æ ‡è¡¨ Schema...")
                target_column_types = {
                    name: type_str for name, type_str in client.execute(
                        f"SELECT name, type FROM system.columns WHERE database = '{db_name}' AND table = '{table_name}'"
                    )
                }

                print("  - ä» SQLite ä¸­æå–æ•°æ®...")
                source_cur.execute(f'SELECT * FROM `{table_name.strip("`[]")}`')
                
                original_column_names = [desc[0] for desc in source_cur.description]
                rows = source_cur.fetchall()
                
                column_names = list(original_column_names)
                
                try:
                    rowid_index = column_names.index('rowid')
                    print("  - æ£€æµ‹åˆ°å¹¶ç§»é™¤éšå¼çš„ 'rowid' åˆ—ã€‚")
                    del column_names[rowid_index]
                    processed_rows = [list(row) for row in rows]
                    for row in processed_rows:
                        del row[rowid_index]
                except ValueError:
                    processed_rows = [list(row) for row in rows]

                vec_info_for_table = vec_tables_info.get(table_name)
                if vec_info_for_table:
                    print("  - æ­£åœ¨è§£ç  sqlite-vec å‘é‡æ•°æ®...")
                    vector_column_indices = {
                        i: item['dim'] 
                        for i, name in enumerate(column_names) 
                        for item in vec_info_for_table['columns'] if item['name'] == name
                    }
                    for row in processed_rows:
                        for i, dim in vector_column_indices.items():
                            blob = row[i]
                            if isinstance(blob, bytes):
                                num_floats = len(blob) // 4
                                if num_floats == dim:
                                    row[i] = list(struct.unpack(f'<{dim}f', blob))
                                else:
                                    print(f"  ğŸŸ¡ è­¦å‘Š: åœ¨è¡¨ '{table_name}' ä¸­å‘ç°ä¸åŒ¹é…çš„å‘é‡ç»´åº¦ã€‚é¢„æœŸ {dim}ï¼Œå¾—åˆ° {num_floats}ã€‚å°†å¡«å……æˆ–æˆªæ–­ã€‚")
                                    unpacked_data = list(struct.unpack(f'<{num_floats}f', blob))
                                    row[i] = (unpacked_data + [0.0] * dim)[:dim]
                
                # --- æ ¸å¿ƒä¿®æ­£ï¼šæ ¹æ® ClickHouse çš„ Schema å¼ºåˆ¶è½¬æ¢æ•°æ®ç±»å‹ ---
                print("  - æ­£åœ¨æ ¹æ®ç›®æ ‡ Schema å¼ºåˆ¶è½¬æ¢æ•°æ®ç±»å‹...")
                final_data = []
                for row in processed_rows:
                    coerced_row = list(row)
                    for i, col_name in enumerate(column_names):
                        target_type = target_column_types.get(col_name)
                        value = coerced_row[i]
                        
                        # å¦‚æœç›®æ ‡åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œè€Œå½“å‰å€¼ä¸æ˜¯å­—ç¬¦ä¸²ï¼ˆä¸”ä¸ä¸º Noneï¼‰ï¼Œåˆ™å¼ºåˆ¶è½¬æ¢
                        if target_type and 'String' in target_type and not isinstance(value, str) and value is not None:
                            coerced_row[i] = str(value)
                            
                    final_data.append(tuple(coerced_row))

                if not final_data:
                    print(f"  - è¡¨ '{table_name}' ä¸ºç©ºï¼Œè·³è¿‡æ•°æ®æ’å…¥ã€‚")
                    continue
                    
                print(f"  - æ‰¹é‡æ’å…¥ {len(final_data)} æ¡æ•°æ®åˆ° ClickHouse...")
                insert_statement = f"INSERT INTO `{table_name}` ({', '.join([f'`{c}`' for c in column_names])}) VALUES"
                client.execute(insert_statement, final_data)
                print(f"  âœ” è¡¨ '{table_name}' æ•°æ®è¿ç§»å®Œæˆã€‚")
        print("\nğŸ‰ æ‰€æœ‰è¡¨è¿ç§»æˆåŠŸï¼")

    except Exception as e:
        print(f"\nâœ– è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if client:
            client.disconnect()

def main():
    parser = argparse.ArgumentParser(description="ä¸€é”®å°† SQLite æ•°æ®åº“ï¼ˆæ”¯æŒ sqlite-vecï¼‰è¿ç§»åˆ° PostgreSQL æˆ– ClickHouseã€‚")
    
    parser.add_argument('--source', default='/mnt/b_public/data/wangzr/Text2VectorSQL/synthesis/toy_spider/results/vector_databases_toy/musical/musical.sqlite', help="æº SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (.db æˆ– .sqlite)ã€‚")
    parser.add_argument('--target', default='clickhouse', choices=['postgresql', 'clickhouse'], help="ç›®æ ‡æ•°æ®åº“ç±»å‹ã€‚")
    parser.add_argument('--host', default='localhost', help="ç›®æ ‡æ•°æ®åº“ä¸»æœºåœ°å€ã€‚")
    parser.add_argument('--user', default='default', help="ç›®æ ‡æ•°æ®åº“ç”¨æˆ·åã€‚")
    parser.add_argument('--password', default='', help="ç›®æ ‡æ•°æ®åº“å¯†ç ã€‚")
    parser.add_argument('--port', default=9000, type=int, help="ç›®æ ‡æ•°æ®åº“ç«¯å£ (PostgreSQL é»˜è®¤ä¸º 5432, ClickHouse é»˜è®¤ä¸º 9000)ã€‚")
    
    args = parser.parse_args()

    if args.port is None:
        args.port = 5432 if args.target == 'postgresql' else 9000

    if not os.path.exists(args.source):
        print(f"âœ– é”™è¯¯ï¼šæºæ–‡ä»¶ '{args.source}' ä¸å­˜åœ¨ã€‚")
        return

    # ä¿®æ­£ï¼šos.path.splitext è¿”å›ä¸€ä¸ªå…ƒç»„ (root, ext)ï¼Œæˆ‘ä»¬éœ€è¦ç¬¬ä¸€ä¸ªå…ƒç´ 
    db_name = os.path.splitext(os.path.basename(args.source))[0]
    # ä¿®æ­£ï¼šç¡®ä¿ db_name æ˜¯å­—ç¬¦ä¸²åå†è¿›è¡Œ re.sub æ“ä½œ
    db_name = re.sub(r'[^a-zA-Z0-9_]', '_', str(db_name))
    db_name = db_name.lower() # æ•°æ®åº“åé€šå¸¸å»ºè®®å°å†™

    print(f"æºæ–‡ä»¶: {args.source}")
    print(f"ç›®æ ‡ç±»å‹: {args.target}")
    print(f"ç›®æ ‡ä¸»æœº: {args.host}:{args.port}")
    print(f"è‡ªåŠ¨åˆ›å»º/ä½¿ç”¨çš„æ•°æ®åº“å: {db_name}")

    if args.target == 'postgresql':
        migrate_to_postgres(args, db_name)
    elif args.target == 'clickhouse':
        migrate_to_clickhouse(args, db_name)

if __name__ == '__main__':
    main()